{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnbd9oNyOy_A"
   },
   "source": [
    "# Sequence to Sequence + Attention Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28rgR7alNu8s"
   },
   "source": [
    "## Table of Contents\n",
    "---\n",
    "\n",
    "* [Problem](#Problem)\n",
    "* [Solution](#Solution)\n",
    "    * [Logistics](#1.-Logistics)\n",
    "    * [Data preprocessing](#2.-Data-Preprocessing)  \n",
    "        * [Training & Validate Data Preperation](#Training-&-Validate-Data-Preparation)\n",
    "        * [Test Data Exploration](#Test-Data-Exploration)\n",
    "    * [Model](#3.-Model)  \n",
    "        * [Architecture of Encoder](#Architecture-of-Encoder)  \n",
    "        * [Architecture of Decoder: Vanilla LSTM](#Architecture-of-Decoder:-Vanilla-LSTM)  \n",
    "        * [Architecture of Decoder: LSTM with Attention](#Architecture-of-Decoder:-LSTM-+-Attention)    \n",
    "    * [Training](#4.-Training)\n",
    "        * [Encoder + Vanilla LSTM Decoder](#Train-the-Model:-Vanilla-Seq2Seq)\n",
    "        * [Encoder + LSTM with Attention Decoder](#Train-the-Model:-Seq2Seq-+-Attention)\n",
    "    * [Evaluation](#5.-Evaluation)\n",
    "        * [With validation data](#With-validation-data)\n",
    "        * [With test data](#With-test-data)\n",
    "    * [Analysis & Discussion](#6.-Analysis-&-Discussion)\n",
    "\n",
    "**Note**: *Some of code snippets are inherited and extended from Gustavo's sample-by-sample Seq2Seq model. I really appreciate his sharing about Attention model. It is very informative and explained in a very easy-to-understand way.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAM7L49fK0hQ"
   },
   "source": [
    "## Problem\n",
    "\n",
    "Build a Seq2Seq + Attention model and compare with vanilla Seq2Seq LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_iMUzOQcwzR"
   },
   "source": [
    "### 1. Logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4PwryCNcu2F"
   },
   "outputs": [],
   "source": [
    "# !pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8MJ8D6YfOy_B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from google.colab import drive\n",
    "from torchviz import make_dot\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "w61MS61cqtPl",
    "outputId": "88ca1756-1d11-482b-a4b3-835c7965bf7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3eF_JUYeq5Rz",
    "outputId": "1698f130-b448-4dbe-b3ac-c7635e6d9b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Model Seq2Seq existed\n",
      "Note: Model Seq2Seq+Attention existed\n",
      "This notebook is using: cuda\n"
     ]
    }
   ],
   "source": [
    "# Normally, the model won't be retrained as model.pt file existed\n",
    "# This constant forces to retrain the model\n",
    "FORCE_RETRAIN = False\n",
    "\n",
    "MODEL_PATH = '/content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output'\n",
    "MODEL_PATH_SEQ2SEQ = '%s/seq2seq.pt' % MODEL_PATH\n",
    "MODEL_PATH_SEQ2SEQ_ATTENTION = '%s/seq2seq+attn.pt' % MODEL_PATH\n",
    "MODEL_PATH_SEQ2SEQ_LOSS = '%s/seq2seq.loss.pt' % MODEL_PATH\n",
    "\n",
    "MODEL_EXISTED_SEQ2SEQ = os.path.isfile(MODEL_PATH_SEQ2SEQ)\n",
    "MODEL_EXISTED_SEQ2SEQ_ATTENTION = os.path.isfile(MODEL_PATH_SEQ2SEQ_ATTENTION)\n",
    "\n",
    "TEST_DATA_PATH = '/content/drive/My Drive/Colab Notebooks/data/NLP/HW3/data/test.txt'\n",
    "\n",
    "if MODEL_EXISTED_SEQ2SEQ:\n",
    "    print('Note: Model Seq2Seq existed')\n",
    "\n",
    "if MODEL_EXISTED_SEQ2SEQ_ATTENTION:\n",
    "    print('Note: Model Seq2Seq+Attention existed')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('This notebook is using: %s' % DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EK4IdHuEOy_E"
   },
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYO13C8WPr4D"
   },
   "source": [
    "#### Training & Validate Data Preparation\n",
    "Our automatically-generated data will contain sequences with repeated and unordered letters. Such sequences must be mapped to alphabetically-sorted sequences of unique letters. Here are a few examples:\n",
    "```\n",
    "ccccaaabb        ->   abc\n",
    "vvvrxduuu        ->   druvx\n",
    "sddvvvzzuuuxxx   ->   dsuvxz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qykhz9toPKZF"
   },
   "outputs": [],
   "source": [
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 10)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXYuTHUTOy_H"
   },
   "source": [
    "We will have two splits in our data, one for **training**, and another for validation. The training set will be **20,000** samples and we will use this set to update the parameters of the model. The **validation** set will be **5,000** samples, which we use to select the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmPfjrNaOy_I"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.itos = vocab\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos) \n",
    "    \n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<start>', '<stop>'] )\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(20_000)\n",
    "valid_inp, valid_out = sorting_letters_dataset(5_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ttff51vOy_K"
   },
   "source": [
    "Now, we need to map the text data into numeric values. These numeric values are indexes that correspond to the entries in the embedding lookup table.\n",
    "    \n",
    "**Batch Processing**  \n",
    "As we need to implement batch processing to our model. \n",
    "We need to make sure every input and output has the same length. Therefore, we add padding token, annotated as `<pad>`, and ending token, annotated as `<stop>` by the end of each sequence. A token `<start>` is also used in the vocabulary set of output to annotate the starting point of a sequence.   \n",
    "\n",
    "As output is a ordered character-deduplicated version of input, output is much shorter than input. So that, we use different length of sequence to adapt the nature of data. This variance of length helps to reduce the redundant computation if using the same length for input and output. In this case, we have the max length of input is **28** and max length of output is **10**.  \n",
    "\n",
    "In addition, this process also allows to extend the maximum length of the sequence as the test data might have different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cWCc9y77Oy_L",
    "outputId": "84b7b893-9419-4f34-aff0-4defbd67d9d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Input Length: 28\n",
      "Max Output Length: 10\n"
     ]
    }
   ],
   "source": [
    "START_IX = tgt_vocab.stoi['<start>']\n",
    "STOP_IX  = tgt_vocab.stoi['<stop>']\n",
    "\n",
    "def max_len(elems):\n",
    "  return max([len(i) for i in elems])\n",
    "\n",
    "# SEQ_LENGTH = max(max_len(train_inp), max_len(valid_inp))\n",
    "SEQ_INPUT_LENGTH = max_len(train_inp)\n",
    "SEQ_OUTPUT_LENGTH = max_len(train_out)\n",
    "\n",
    "print('Max Input Length:', SEQ_INPUT_LENGTH)\n",
    "print('Max Output Length:', SEQ_OUTPUT_LENGTH)\n",
    "\n",
    "def map_elems(elems, mapper, seq_type='input', seq_length=None):\n",
    "    if seq_type == 'input':\n",
    "        pad_index = mapper['<pad>']\n",
    "        max_length = seq_length or SEQ_INPUT_LENGTH\n",
    "        assert len(elems) <= max_length, (\"Invalid string: %s\" % str(elems))\n",
    "        return [mapper[elem] for elem in elems] + ([pad_index] * (max_length - len(elems)))\n",
    "    elif seq_type == 'output':\n",
    "        pad_index = STOP_IX\n",
    "        max_length = seq_length or SEQ_OUTPUT_LENGTH\n",
    "        assert len(elems) <= max_length, (\"Invalid string: %s\" % str(elems))\n",
    "        return [mapper[elem] for elem in elems] + ([pad_index] * (max_length - len(elems)))\n",
    "\n",
    "def map_many_elems(many_elems, mapper, seq_type='input', seq_length=None):\n",
    "    return [map_elems(elems, mapper, seq_type=seq_type, seq_length=seq_length) for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi, seq_type='input')\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi, seq_type='output')\n",
    "\n",
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi, seq_type='input')\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi, seq_type='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1fIdUCFrwx5x",
    "outputId": "24710307-abf3-44e9-c84c-7951001c4caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'i', 'p', 'p', 'p', 'p', 'p', 'm']\n",
      "[19, 9, 16, 16, 16, 16, 16, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['i', 'm', 'p', 's']\n",
      "[9, 13, 16, 19, 28, 28, 28, 28, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "# Validate the data\n",
    "print(train_inp[0])\n",
    "print(train_x[0])\n",
    "assert len(train_x[0]) == SEQ_INPUT_LENGTH, \"Invalid length\"\n",
    "\n",
    "print(train_out[0])\n",
    "print(train_y[0])\n",
    "assert len(train_y[0]) == SEQ_OUTPUT_LENGTH, \"Invalid length\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RtwH-Wj4QccN"
   },
   "source": [
    "#### Test Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gwGoLtf7pZxn"
   },
   "source": [
    "As we see here the length of Test data (**58**) is much higher than what we have in training and validating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "_QEs29t-Oy_x",
    "outputId": "dc506c23-a2c4-471e-8ea8-6a15b469e063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test cases: 5000\n",
      "Max length of input: 58\n",
      "Min length of input: 3\n",
      "Max length of output: 13\n",
      "Min length of output: 2\n",
      "Max repeated length: 8.0 (In=lllllllfffflllll, Out=fl)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv(TEST_DATA_PATH, delimiter='\\t', header=None, usecols=[0,1])\n",
    "test_inp, test_out = test_data[0], test_data[1]\n",
    "\n",
    "print('Total test cases: %s' % len(test_inp))\n",
    "print('Max length of input: %s' % max([len(i) for i in test_inp]))\n",
    "print('Min length of input: %s' % min([len(i) for i in test_inp]))\n",
    "print('Max length of output: %s' % max([len(o) for o in test_out]))\n",
    "print('Min length of output: %s' % min([len(o) for o in test_out]))\n",
    "print('Max repeated length: %s (In=%s, Out=%s)' % (max([len(w)/len(test_out[idx]) for idx, w in enumerate(test_inp)]), \n",
    "                                              test_inp[[len(w)/len(test_out[idx]) for idx, w in enumerate(test_inp)].index(8)],\n",
    "                                              test_out[[len(w)/len(test_out[idx]) for idx, w in enumerate(test_inp)].index(8)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fr6HiOvVOy_N"
   },
   "source": [
    "### 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjbHkwITR9Fj"
   },
   "source": [
    "#### Architecture of Encoder\n",
    "Our encoder will be a simple LSTM model with one layer and one direction. For batch processing, a batch of same-size sequences are fed in the encoder to generate the encoder states.\n",
    "\n",
    "\n",
    "<img src='https://github.com/gaguilar/basic_nlp_tutorial/blob/master/tutorial_on_seq2seq_models/images/encoder.png?raw=1' width='50%'/>\n",
    "\n",
    "<small>Photo Credit: Gustavo's slide</small>\n",
    "\n",
    "**Explanation**\n",
    "- $h_i$ is the short-term state.\n",
    "- $c_i$ is the long-term state.\n",
    "\n",
    "**Two types of outputs**\n",
    "- Last state: $z$ = $h_n$ and $c_n$. This is used in Vanilla Seq2Seq model.\n",
    "- All states: $(h_1, h_2, ..., h_n)$ and $(c_1, c_2, ..., c_n)$. This is used in Seq2Seq + Attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYuYXQpKNxxH"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, z_type, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_index = z_type\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        device = next(self.parameters()).device\n",
    "        seq = torch.tensor(inputs).to(device) # (1, seqlen)\n",
    "        emb = self.emb(seq) # (1, seqlen, emb_dim)\n",
    "        batch_size = seq.size(0)\n",
    "        emb = self.drop(emb) \n",
    "        \n",
    "        outs, (h_n, c_n) = self.lstm(emb)\n",
    "\n",
    "        assert outs.size(0) == batch_size\n",
    "        # assert outs.size(1) == SEQ_INPUT_LENGTH\n",
    "        assert outs.size(2) == self.hidden_dim\n",
    "\n",
    "        if self.z_index == 1:\n",
    "            return h_n[0], c_n[0] # (seqlen, lstm_dim)\n",
    "        else:\n",
    "            return outs # (1, seqlen, lstm_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlH5RYAtuuhf"
   },
   "source": [
    "**Structure of the Encoder**\n",
    "- *Embedding Module*: A batch of sequences is embedded after being fed in the encoder. This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    "- *Dropout Module*: Embedded batch is dropped out. This helps for regularization and preventing the co-adaptation of neurons as described in the paper [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580). During training, randomly zeroes some of the elements of the input tensor with probability $p$ using samples from a $Bernoulli$ distribution.\n",
    "- *Unidirectional LSTM*: LSTM is applied to learn the pattern of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "AQ-WiHCBul_q",
    "outputId": "1a341233-89ca-483a-d838-3d25effaa5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (emb): Embedding(27, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"455pt\" height=\"285pt\"\n",
       " viewBox=\"0.00 0.00 455.00 285.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 281)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-281 451,-281 451,4 -4,4\"/>\n",
       "<!-- 139927861678936 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139927861678936</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"259,-21 165,-21 165,0 259,0 259,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SelectBackward</text>\n",
       "</g>\n",
       "<!-- 139927861681680 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139927861681680</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"327,-78 209,-78 209,-57 327,-57 327,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">CudnnRnnBackward</text>\n",
       "</g>\n",
       "<!-- 139927861681680&#45;&gt;139927861678936 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139927861681680&#45;&gt;139927861678936</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M257.4668,-56.7787C249.645,-48.8173 238.7954,-37.7739 229.6564,-28.4717\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"231.9909,-25.8537 222.4859,-21.1732 226.9975,-30.7595 231.9909,-25.8537\"/>\n",
       "</g>\n",
       "<!-- 139927861678376 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>139927861678376</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"371,-21 277,-21 277,0 371,0 371,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"324\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SelectBackward</text>\n",
       "</g>\n",
       "<!-- 139927861681680&#45;&gt;139927861678376 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>139927861681680&#45;&gt;139927861678376</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M278.5332,-56.7787C286.355,-48.8173 297.2046,-37.7739 306.3436,-28.4717\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"309.0025,-30.7595 313.5141,-21.1732 304.0091,-25.8537 309.0025,-30.7595\"/>\n",
       "</g>\n",
       "<!-- 139927862287888 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139927862287888</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"134,-142 0,-142 0,-121 134,-121 134,-142\"/>\n",
       "<text text-anchor=\"middle\" x=\"67\" y=\"-128.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">FusedDropoutBackward</text>\n",
       "</g>\n",
       "<!-- 139927862287888&#45;&gt;139927861681680 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139927862287888&#45;&gt;139927861681680</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M100.1911,-120.9317C134.3781,-110.0463 187.7967,-93.0374 225.1545,-81.1423\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"226.2929,-84.4531 234.7596,-78.084 224.1691,-77.783 226.2929,-84.4531\"/>\n",
       "</g>\n",
       "<!-- 139927862286992 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139927862286992</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"127,-206 7,-206 7,-185 127,-185 127,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"67\" y=\"-192.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">EmbeddingBackward</text>\n",
       "</g>\n",
       "<!-- 139927862286992&#45;&gt;139927862287888 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139927862286992&#45;&gt;139927862287888</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M67,-184.9317C67,-176.0913 67,-163.2122 67,-152.3135\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"70.5001,-152.2979 67,-142.2979 63.5001,-152.2979 70.5001,-152.2979\"/>\n",
       "</g>\n",
       "<!-- 139927862288168 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139927862288168</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"95.5,-277 38.5,-277 38.5,-242 95.5,-242 95.5,-277\"/>\n",
       "<text text-anchor=\"middle\" x=\"67\" y=\"-249.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (27, 64)</text>\n",
       "</g>\n",
       "<!-- 139927862288168&#45;&gt;139927862286992 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139927862288168&#45;&gt;139927862286992</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M67,-241.6724C67,-233.8405 67,-224.5893 67,-216.4323\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"70.5001,-216.2234 67,-206.2234 63.5001,-216.2235 70.5001,-216.2234\"/>\n",
       "</g>\n",
       "<!-- 139927862286040 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139927862286040</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"215.5,-149 152.5,-149 152.5,-114 215.5,-114 215.5,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"184\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (512, 64)</text>\n",
       "</g>\n",
       "<!-- 139927862286040&#45;&gt;139927861681680 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139927862286040&#45;&gt;139927861681680</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M207.3987,-113.6724C219.4302,-104.5056 234.0138,-93.3942 245.8595,-84.3689\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"248.0923,-87.0679 253.9255,-78.2234 243.85,-81.4999 248.0923,-87.0679\"/>\n",
       "</g>\n",
       "<!-- 139927862287216 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139927862287216</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"302.5,-149 233.5,-149 233.5,-114 302.5,-114 302.5,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (512, 128)</text>\n",
       "</g>\n",
       "<!-- 139927862287216&#45;&gt;139927861681680 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139927862287216&#45;&gt;139927861681680</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M268,-113.6724C268,-105.8405 268,-96.5893 268,-88.4323\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"271.5001,-88.2234 268,-78.2234 264.5001,-88.2235 271.5001,-88.2234\"/>\n",
       "</g>\n",
       "<!-- 139927862287944 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>139927862287944</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"375,-149 321,-149 321,-114 375,-114 375,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"348\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (512)</text>\n",
       "</g>\n",
       "<!-- 139927862287944&#45;&gt;139927861681680 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>139927862287944&#45;&gt;139927861681680</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M325.7155,-113.6724C314.3682,-104.5946 300.6372,-93.6098 289.4155,-84.6324\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"291.3994,-81.7374 281.4043,-78.2234 287.0266,-87.2035 291.3994,-81.7374\"/>\n",
       "</g>\n",
       "<!-- 139927862287104 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>139927862287104</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"447,-149 393,-149 393,-114 447,-114 447,-149\"/>\n",
       "<text text-anchor=\"middle\" x=\"420\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (512)</text>\n",
       "</g>\n",
       "<!-- 139927862287104&#45;&gt;139927861681680 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>139927862287104&#45;&gt;139927861681680</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M392.97,-118.1049C389.9578,-116.6885 386.9229,-115.2943 384,-114 357.9617,-102.4699 328.1315,-90.5398 305.2419,-81.6507\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"306.4777,-78.376 295.8883,-78.0372 303.9551,-84.9057 306.4777,-78.376\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f437de20a58>"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, hidden_dim=128, z_type=1)\n",
    "encoder.to(DEVICE)\n",
    "test_state = encoder(train_x[0:5])\n",
    "\n",
    "print(encoder)\n",
    "make_dot(test_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LP0J3v43Oy_R"
   },
   "source": [
    "#### Architecture of Decoder: Vanilla LSTM\n",
    "\n",
    "Similar to the encoder, the decoder will be a LSTM cell with one layer and one direction. \n",
    "\n",
    "<img src='https://github.com/gaguilar/basic_nlp_tutorial/blob/master/tutorial_on_seq2seq_models/images/decoder.png?raw=1' width='50%'/>\n",
    "\n",
    "<small>Photo Credit: Gustavo's slide</small>\n",
    "\n",
    "**Explanation**\n",
    "- $y_i$ is the actual token in the output sequence.\n",
    "- $q_i$ is the query vector or short-term state at step $i$.\n",
    "- $\\hat y_i$ is the predicted token at step $i$.\n",
    "- $z$ is the state generated from encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PrRUzvTZOuIX"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, hidden_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.clf = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        \n",
    "    def forward(self, state, targets, curr_token, last_token):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        loss = 0\n",
    "        shifted = [t + [last_token] for t in targets]\n",
    "        batch_size = state[0].shape[0]\n",
    "        seq_len = len(targets[0]) + 1\n",
    "\n",
    "        curr_tokens = [curr_token] * batch_size\n",
    "        for i in range(seq_len):\n",
    "            inp = torch.tensor(curr_tokens).to(device)\n",
    "            \n",
    "            emb = self.emb(inp)\n",
    "            emb = self.drop(emb)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state \n",
    "            q_i = self.drop(q_i)\n",
    "\n",
    "            scores = self.clf(q_i)\n",
    "\n",
    "            outs = [t[i] for t in shifted]\n",
    "            target = torch.tensor(outs).to(device)\n",
    "            loss += self.objective(scores, target)   \n",
    "            curr_tokens = outs\n",
    "\n",
    "        return loss / seq_len\n",
    "\n",
    "    def predict(self, state, curr_token, last_token, maxlen=15):\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = state[0].shape[0]\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(maxlen):\n",
    "            inp = torch.tensor([curr_token]).to(device)\n",
    "            emb = self.emb(inp)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            h_i, _ = state\n",
    "            \n",
    "            scores = self.clf(h_i)\n",
    "            pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "            curr_token = pred\n",
    "            \n",
    "            if last_token == pred:\n",
    "                break\n",
    "            preds.append(pred)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nh9LLPwfCIc-"
   },
   "source": [
    "**Structure of the Vanilla LSTM Decoder**\n",
    "- *Embedding Module*: Similar to what we have in the encoder though the input to this module is $z$ state which is generated from encoder.\n",
    "- *LSTM Cell*: LSTM is applied to learn the pattern of the sequence. We use a LSTM cell here and implement since it only return only the last state. So that we can implement a loop of learning with the number of interation is equal to the length of the output sequence.\n",
    "- *Dropout Module*: Similar to what we have in the encoder.\n",
    "- *Linear Module*: To transform the $q_i$ query vector to a vector in vocabulary space.\n",
    "- *Cross Entropy Loss*: To compute the different between prediction and actual token. This module is a combination of both *Log Softmax Loss* and *Negative Log Likelihood Loss*. It is useful when training a classification problem with C classes. In our case, we try to predict a character in a set of vocabulary. Because we are processing batch-by-batch, we enable the reduction step to summarize all the losses of each sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "r7PukMZaB9RY",
    "outputId": "a1cb609d-afed-425c-fdae-a0f5e68c2960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (clf): Linear(in_features=128, out_features=29, bias=True)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVhHXZ9IOy_V"
   },
   "source": [
    "#### Architecture of Decoder: LSTM + Attention\n",
    "\n",
    "The attention version that we use for this implementation is Luong's attention, also known as multiplicative attention. [Effective Approaches to Attention-based Neural Machine Translation\n",
    "Minh-Thang Luong, Hieu Pham, Christopher D. Manning](https://arxiv.org/abs/1508.04025)\n",
    "\n",
    "Consider the encoder outputs $h = [h_1, h_2, \\dots, h_n]$, and the query vector $q_j$ of the decoding time step $j$ as the hidden vector of the decoder LSTM, then we define multiplicative attention as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_i &= v^\\intercal tanh(W [h_i , q_j]) \\\\\n",
    "\\alpha_i &= \\frac{exp(u_i)}{\\sum^N_k exp(u_k)} \\\\\n",
    "c_i &= \\sum^N_i \\alpha_i h_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdCkypRWOy_V"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, attn_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, attn_dim)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outs):\n",
    "        seqlen = enc_outs.size(1)\n",
    "        \n",
    "        repeat_h = dec_hidden.unsqueeze(1)  # make room to repeat on seqlen dim\n",
    "        repeat_h = repeat_h.repeat(1, seqlen, 1)  # (1, seqlen, hidden)\n",
    "\n",
    "        concat_h = torch.cat((enc_outs, repeat_h), dim=2) # (1, seqlen, hidden*2)\n",
    "        \n",
    "        scores = self.v(torch.tanh(self.W(concat_h))) # (1, seqlen, 1)\n",
    "        probs = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        weighted = enc_outs * probs # (1, seqlen, hidden)\n",
    "        \n",
    "        context = torch.sum(weighted, dim=1, keepdim=False) # (1, hidden)\n",
    "        combined = torch.cat((dec_hidden, context), dim=1)  # (1, hidden*2)\n",
    "        \n",
    "        return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sWWjC-D_EdmW"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "    <td><img src=\"https://api.monosnap.com/file/download?id=whilNaVgdGuahVjqqiljSg8RI39ZGe\" width=\"500\" /></td>\n",
    "    <td><img src=\"https://api.monosnap.com/file/download?id=fI5u0x0Pzn0YwTmQ7EogLGs7XM83PA\" width=\"500\" /></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Structure of An Attention Module**  \n",
    "- At each step $i$ in the decoder we have a query vector $q_i$.\n",
    "- At each step $j$ in the encoder we also have a short-term state $h_j$.\n",
    "- To identify which steps/states in the encoder that we should pay more attention, we use Luong's (multiplicative) score function then use a softmax function to compute the corresponding contribution (weight $\\alpha_i$) of each state.\n",
    "- Sum of the weighted stated we have a proper context $c_i$.\n",
    "- Finally, combine the context $c_i$ with the query vector $q_i$, we have a new query vector with attention $q_{i_{combine}} = [q_i, c_i]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Emk7KppYEWPM",
    "outputId": "4f3cc623-2b1a-472c-d4d3-a3191500a295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention(\n",
      "  (W): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (v): Linear(in_features=64, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "attention = Attention(256, 64)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oF1pdztpOy_X"
   },
   "source": [
    "Since we want the decoder to focus on the right hidden outputs of the encoder, we need the to modify the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6hldX8dOy_Y"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, attn_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, hidden_dim)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.attn = Attention(hidden_dim * 2, attn_size)\n",
    "        self.clf = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        \n",
    "    def init_state(self, device, batch_size=1):\n",
    "        h_0 = torch.zeros(batch_size, self.hidden_dim).to(device)  # (batch, hidden_size)\n",
    "        c_0 = torch.zeros(batch_size, self.hidden_dim).to(device)  # (batch, hidden_size)\n",
    "        return h_0, c_0\n",
    "        \n",
    "    def forward(self, enc_outs, targets, curr_token, last_token):\n",
    "        loss = 0\n",
    "        batch_size = enc_outs.size(0)\n",
    "        device = enc_outs.device\n",
    "\n",
    "        state = self.init_state(device, batch_size=batch_size)\n",
    "        shifted = [t + [last_token] for t in targets]\n",
    "        curr_tokens = [curr_token] * batch_size\n",
    "        seq_len = len(targets[0]) + 1\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            inp = torch.tensor(curr_tokens).to(device) # (1,)\n",
    "            \n",
    "            emb = self.emb(inp) # (1, emb_dim)\n",
    "            emb = self.drop(emb)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state \n",
    "            q_i = self.drop(q_i) # (1, emb_dim)\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs)     \n",
    "            scores = self.clf(combined)\n",
    "\n",
    "            outs = [t[i] for t in shifted]\n",
    "            target = torch.tensor(outs).to(device)\n",
    "\n",
    "            loss += self.objective(scores, target)\n",
    "            \n",
    "            curr_tokens = outs\n",
    "            \n",
    "        return loss / seq_len\n",
    "\n",
    "    def predict(self, enc_outs, curr_token, last_token, maxlen):\n",
    "        preds = []\n",
    "        device = enc_outs.device\n",
    "        state = self.init_state(device)\n",
    "        \n",
    "        for i in range(maxlen):\n",
    "            inp = torch.tensor([curr_token]).to(device)\n",
    "            emb = self.emb(inp)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs)\n",
    "            \n",
    "            scores = self.clf(combined)\n",
    "            pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "            curr_token = pred\n",
    "            \n",
    "            if last_token == pred:\n",
    "                break\n",
    "                \n",
    "            preds.append(pred)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPXeFUIQD10O"
   },
   "source": [
    "**Structure of Attention Decoder**\n",
    "\n",
    "<img src=\"https://api.monosnap.com/file/download?id=I2mEObBRNHaQs1ybxxZitbb5QjTLqQ\" width=\"500\"/>\n",
    "\n",
    "- All other modules are same as in the Vanilla LSTM Decoder. The only additional module is Attention Module.\n",
    "- *Attention Module*: The only difference between Attention Decoder and Attention module is the query vector $q_i$ that is used to predict the token. While $q_i$ is passed directly to the *Linear Module* in Vanilla LSTM, it needs to go through the *Attention Module* and transform into $q_{i_{combine}} = [q_i, c_i]$ with Attention mechanism before passing to the *Linear Module*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "XP7pncwNDzU-",
    "outputId": "0055d934-b864-4eac-81eb-bdd867d22d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionDecoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (attn): Attention(\n",
      "    (W): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (v): Linear(in_features=64, out_features=1, bias=False)\n",
      "  )\n",
      "  (clf): Linear(in_features=256, out_features=29, bias=True)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128, attn_size=64)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GUKqsWhfY1Qy"
   },
   "source": [
    "#### Test to make sure each unit of the whole model work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ceHWJsqVb7A5"
   },
   "source": [
    "##### **Test Encoder + Decoder: Vanilla LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCHsvHkCiZE0"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, hidden_dim=128, z_type=1)\n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128)\n",
    "# encoder\n",
    "# decoder\n",
    "\n",
    "# Map to CPU or GPU\n",
    "encoder.to(DEVICE)\n",
    "decoder.to(DEVICE)\n",
    "\n",
    "test_state = encoder(train_x[0:2])\n",
    "test_loss = decoder(test_state, train_y[0:2], START_IX, STOP_IX)\n",
    "\n",
    "# make_dot(test_state)\n",
    "# make_dot(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v481CwKncASm"
   },
   "source": [
    "##### **Test Encoder + Decoder: LSTM + Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8Mm26v72Kf8"
   },
   "outputs": [],
   "source": [
    "# Test Encoder + Decoder: LSTM + Attention\n",
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, hidden_dim=128, z_type=0)\n",
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128, attn_size=64)\n",
    "\n",
    "encoder.to(DEVICE)\n",
    "decoder.to(DEVICE)\n",
    "\n",
    "test_state = encoder(train_x[0:2])\n",
    "test_loss = decoder(test_state, train_y[0:2], START_IX, STOP_IX)\n",
    "\n",
    "# make_dot(test_state)\n",
    "# make_dot(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obIcv4FZdlBg"
   },
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYIaTzQCEpo4"
   },
   "source": [
    "Steps of training:\n",
    "- For each epoch in $n$ epoches\n",
    "    - Shuffle training data\n",
    "    - Train the encoder and decoder with batch-by-batch data (50 samples per batch) from training data set. Optimizers are used in this training process is SGD. (Still think about other optimizers that are more appropriate)\n",
    "    - Evalulate with validating data.\n",
    "    - Compare the evaluating loss with the bess loss so far, if the new loss is lesser. Store model at that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjC7dGEqhYOL"
   },
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51SxA03GOy_a"
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, train_x, train_y, epoch, batch_size=50, print_every=1):\n",
    "    x = []\n",
    "    y = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    enc_optim = optim.SGD(encoder.parameters(), lr=0.001, momentum=0.99)\n",
    "    dec_optim = optim.SGD(decoder.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    encoder.zero_grad(); enc_optim.zero_grad()\n",
    "    decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    batch_loss = 0    \n",
    "    \n",
    "    for i in range(len(train_x)):\n",
    "        x.append(train_x[i]) \n",
    "        y.append(train_y[i])\n",
    "\n",
    "        \n",
    "\n",
    "        if (i+1) % batch_size == 0:\n",
    "            encoded = encoder(x)\n",
    "            batch_loss = decoder(encoded, y, START_IX, STOP_IX)\n",
    "            batch_loss.backward()\n",
    "            \n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "            encoder.zero_grad(); enc_optim.zero_grad()\n",
    "            decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            batch_loss = 0\n",
    "            x = []\n",
    "            y = []\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"[TRAIN] Epoch {epoch} - Loss: {epoch_loss / len(train_x):.6f}\")\n",
    "    return encoder, decoder, epoch_loss / len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EtiIaM0IJJHq"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, valid_x, valid_y, epoch, batch_size=50, print_every=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    x = []\n",
    "    y = []\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    batch_loss = 0    \n",
    "    \n",
    "    for i in range(len(valid_x)):\n",
    "        x.append(valid_x[i]) \n",
    "        y.append(valid_y[i])\n",
    "\n",
    "        if (i+1) % batch_size == 0:\n",
    "            encoded = encoder(x)\n",
    "            batch_loss = decoder(encoded, y, START_IX, STOP_IX)\n",
    "            \n",
    "            epoch_loss += batch_loss.item()\n",
    "            batch_loss = 0\n",
    "            x = []\n",
    "            y = []\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"[EVAL] Epoch {epoch} - Loss: {epoch_loss / len(valid_x):.6f}\")\n",
    "    return encoder, decoder, epoch_loss / len(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V45wyvkCJOWP"
   },
   "outputs": [],
   "source": [
    "def track_best_model(model_path, encoder, decoder, epoch, best_loss, new_loss, valid_x_len, losses=None):\n",
    "    if best_loss != None and best_loss < new_loss:\n",
    "        return False\n",
    "    \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'loss': new_loss,\n",
    "        'losses': losses,\n",
    "        'encoder': encoder.state_dict(),\n",
    "        'decoder': decoder.state_dict()\n",
    "    }\n",
    "    torch.save(state, model_path)\n",
    "    print('[*] Stored best model to %s' % model_path)\n",
    "    return new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJqB6sq07jL_"
   },
   "outputs": [],
   "source": [
    "def train_loop(encoder, decoder, train_x, train_y, \n",
    "               validate=False, valid_x=None, valid_y=None,\n",
    "               verbose=True, epochs=100, patient_epochs=10, \n",
    "               early_stop=False, threshold=0.01, print_every=1,\n",
    "               store_best_model=False, store_after_epochs=1,\n",
    "               store_path=None):\n",
    "    \n",
    "    best_loss = None\n",
    "    train_loss = None\n",
    "    valid_loss = None\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # TODO: Implement early stop\n",
    "    patient_count = 0\n",
    "\n",
    "    if verbose:\n",
    "        print(encoder)\n",
    "        print(decoder)\n",
    "\n",
    "    print('Start: ', datetime.now())\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder, decoder, train_loss = train(encoder, decoder, train_x, train_y, epoch, batch_size = 50, print_every=1)\n",
    "        \n",
    "        if validate:\n",
    "             encoder, decoder, valid_loss = evaluate(encoder, decoder, valid_x, valid_y, epoch, batch_size=50, print_every=1)\n",
    "        \n",
    "        if store_best_model:\n",
    "            assert store_path is not None, \"Please provide model path for storing\"\n",
    "\n",
    "            losses = {\n",
    "                'train_losses': train_losses + [train_loss],\n",
    "                'valid_losses': valid_losses + [valid_loss] if validate else valid_losses\n",
    "            }\n",
    "            if epoch > store_after_epochs:\n",
    "                if validate:\n",
    "                    best_loss = min(valid_losses) if valid_losses else 2e32\n",
    "                    track_best_model(store_path, encoder, decoder, epoch, best_loss, valid_loss, len(valid_x), losses=losses)\n",
    "                else:\n",
    "                    best_loss = min(train_losses) if train_losses else 2e32\n",
    "                    track_best_model(store_path, encoder, decoder, epoch, best_loss, train_loss, len(train_x), losses=losses) \n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        if validate:\n",
    "            valid_losses.append(valid_loss)\n",
    "    \n",
    "    print('Start: ', datetime.now())\n",
    "    return encoder, decoder, train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07gsz8-YOy_a"
   },
   "source": [
    "#### Train the Model: Vanilla Seq2Seq\n",
    "\n",
    "During training, we pass the targets to the decoder so that it can be used as the ideal input at time step $i$, instead of using the decoder predictions of the previous time step $i-1$.\n",
    "\n",
    "<img src='https://github.com/gaguilar/basic_nlp_tutorial/blob/master/tutorial_on_seq2seq_models/images/encoder-decoder-lstm.png?raw=1' width='80%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s2xqxtlTJVaa",
    "outputId": "cc26e76a-91fd-4c7e-f555-5629f6f4ed1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (emb): Embedding(27, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Decoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (clf): Linear(in_features=128, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n",
      "Start:  2020-04-10 19:37:42.498339\n",
      "[TRAIN] Epoch 1 - Loss: 1.517220\n",
      "[EVAL] Epoch 1 - Loss: 1.277119\n",
      "[TRAIN] Epoch 2 - Loss: 1.328496\n",
      "[EVAL] Epoch 2 - Loss: 1.266918\n",
      "[TRAIN] Epoch 3 - Loss: 1.308332\n",
      "[EVAL] Epoch 3 - Loss: 1.263388\n",
      "[TRAIN] Epoch 4 - Loss: 1.300881\n",
      "[EVAL] Epoch 4 - Loss: 1.259731\n",
      "[TRAIN] Epoch 5 - Loss: 1.293016\n",
      "[EVAL] Epoch 5 - Loss: 1.259490\n",
      "[TRAIN] Epoch 6 - Loss: 1.290775\n",
      "[EVAL] Epoch 6 - Loss: 1.257853\n",
      "[TRAIN] Epoch 7 - Loss: 1.285655\n",
      "[EVAL] Epoch 7 - Loss: 1.256486\n",
      "[TRAIN] Epoch 8 - Loss: 1.283903\n",
      "[EVAL] Epoch 8 - Loss: 1.258542\n",
      "[TRAIN] Epoch 9 - Loss: 1.281834\n",
      "[EVAL] Epoch 9 - Loss: 1.256034\n",
      "[TRAIN] Epoch 10 - Loss: 1.278887\n",
      "[EVAL] Epoch 10 - Loss: 1.252474\n",
      "[TRAIN] Epoch 11 - Loss: 1.266413\n",
      "[EVAL] Epoch 11 - Loss: 1.215951\n",
      "[TRAIN] Epoch 12 - Loss: 1.228477\n",
      "[EVAL] Epoch 12 - Loss: 1.184131\n",
      "[TRAIN] Epoch 13 - Loss: 1.211007\n",
      "[EVAL] Epoch 13 - Loss: 1.171459\n",
      "[TRAIN] Epoch 14 - Loss: 1.180569\n",
      "[EVAL] Epoch 14 - Loss: 1.107141\n",
      "[TRAIN] Epoch 15 - Loss: 1.137366\n",
      "[EVAL] Epoch 15 - Loss: 1.078564\n",
      "[TRAIN] Epoch 16 - Loss: 1.096385\n",
      "[EVAL] Epoch 16 - Loss: 1.013817\n",
      "[TRAIN] Epoch 17 - Loss: 1.040587\n",
      "[EVAL] Epoch 17 - Loss: 0.970235\n",
      "[TRAIN] Epoch 18 - Loss: 1.005432\n",
      "[EVAL] Epoch 18 - Loss: 0.951383\n",
      "[TRAIN] Epoch 19 - Loss: 0.981860\n",
      "[EVAL] Epoch 19 - Loss: 0.918729\n",
      "[TRAIN] Epoch 20 - Loss: 0.942290\n",
      "[EVAL] Epoch 20 - Loss: 0.854795\n",
      "[TRAIN] Epoch 21 - Loss: 0.878190\n",
      "[EVAL] Epoch 21 - Loss: 0.767103\n",
      "[TRAIN] Epoch 22 - Loss: 0.813481\n",
      "[EVAL] Epoch 22 - Loss: 0.728267\n",
      "[TRAIN] Epoch 23 - Loss: 0.774922\n",
      "[EVAL] Epoch 23 - Loss: 0.685008\n",
      "[TRAIN] Epoch 24 - Loss: 0.732252\n",
      "[EVAL] Epoch 24 - Loss: 0.650280\n",
      "[TRAIN] Epoch 25 - Loss: 0.692915\n",
      "[EVAL] Epoch 25 - Loss: 0.590967\n",
      "[TRAIN] Epoch 26 - Loss: 0.655893\n",
      "[EVAL] Epoch 26 - Loss: 0.576928\n",
      "[TRAIN] Epoch 27 - Loss: 0.627249\n",
      "[EVAL] Epoch 27 - Loss: 0.540866\n",
      "[TRAIN] Epoch 28 - Loss: 0.590893\n",
      "[EVAL] Epoch 28 - Loss: 0.524513\n",
      "[TRAIN] Epoch 29 - Loss: 0.569489\n",
      "[EVAL] Epoch 29 - Loss: 0.510711\n",
      "[TRAIN] Epoch 30 - Loss: 0.553573\n",
      "[EVAL] Epoch 30 - Loss: 0.496115\n",
      "[TRAIN] Epoch 31 - Loss: 0.535576\n",
      "[EVAL] Epoch 31 - Loss: 0.489664\n",
      "[TRAIN] Epoch 32 - Loss: 0.525485\n",
      "[EVAL] Epoch 32 - Loss: 0.478582\n",
      "[TRAIN] Epoch 33 - Loss: 0.507947\n",
      "[EVAL] Epoch 33 - Loss: 0.462443\n",
      "[TRAIN] Epoch 34 - Loss: 0.490100\n",
      "[EVAL] Epoch 34 - Loss: 0.433681\n",
      "[TRAIN] Epoch 35 - Loss: 0.470046\n",
      "[EVAL] Epoch 35 - Loss: 0.422451\n",
      "[TRAIN] Epoch 36 - Loss: 0.451299\n",
      "[EVAL] Epoch 36 - Loss: 0.393277\n",
      "[TRAIN] Epoch 37 - Loss: 0.431885\n",
      "[EVAL] Epoch 37 - Loss: 0.373099\n",
      "[TRAIN] Epoch 38 - Loss: 0.408137\n",
      "[EVAL] Epoch 38 - Loss: 0.357338\n",
      "[TRAIN] Epoch 39 - Loss: 0.395537\n",
      "[EVAL] Epoch 39 - Loss: 0.350537\n",
      "[TRAIN] Epoch 40 - Loss: 0.384331\n",
      "[EVAL] Epoch 40 - Loss: 0.339177\n",
      "[TRAIN] Epoch 41 - Loss: 0.375792\n",
      "[EVAL] Epoch 41 - Loss: 0.336229\n",
      "[TRAIN] Epoch 42 - Loss: 0.370475\n",
      "[EVAL] Epoch 42 - Loss: 0.328670\n",
      "[TRAIN] Epoch 43 - Loss: 0.364531\n",
      "[EVAL] Epoch 43 - Loss: 0.312675\n",
      "[TRAIN] Epoch 44 - Loss: 0.345822\n",
      "[EVAL] Epoch 44 - Loss: 0.282665\n",
      "[TRAIN] Epoch 45 - Loss: 0.320906\n",
      "[EVAL] Epoch 45 - Loss: 0.257913\n",
      "[TRAIN] Epoch 46 - Loss: 0.300259\n",
      "[EVAL] Epoch 46 - Loss: 0.239325\n",
      "[TRAIN] Epoch 47 - Loss: 0.284887\n",
      "[EVAL] Epoch 47 - Loss: 0.226779\n",
      "[TRAIN] Epoch 48 - Loss: 0.264572\n",
      "[EVAL] Epoch 48 - Loss: 0.195540\n",
      "[TRAIN] Epoch 49 - Loss: 0.242198\n",
      "[EVAL] Epoch 49 - Loss: 0.182294\n",
      "[TRAIN] Epoch 50 - Loss: 0.231169\n",
      "[EVAL] Epoch 50 - Loss: 0.180430\n",
      "[TRAIN] Epoch 51 - Loss: 0.222204\n",
      "[EVAL] Epoch 51 - Loss: 0.173055\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 52 - Loss: 0.210342\n",
      "[EVAL] Epoch 52 - Loss: 0.168590\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 53 - Loss: 0.208671\n",
      "[EVAL] Epoch 53 - Loss: 0.163282\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 54 - Loss: 0.205683\n",
      "[EVAL] Epoch 54 - Loss: 0.164373\n",
      "[TRAIN] Epoch 55 - Loss: 0.200164\n",
      "[EVAL] Epoch 55 - Loss: 0.155430\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 56 - Loss: 0.197127\n",
      "[EVAL] Epoch 56 - Loss: 0.156970\n",
      "[TRAIN] Epoch 57 - Loss: 0.201461\n",
      "[EVAL] Epoch 57 - Loss: 0.170474\n",
      "[TRAIN] Epoch 58 - Loss: 0.198417\n",
      "[EVAL] Epoch 58 - Loss: 0.159789\n",
      "[TRAIN] Epoch 59 - Loss: 0.191257\n",
      "[EVAL] Epoch 59 - Loss: 0.154609\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 60 - Loss: 0.190631\n",
      "[EVAL] Epoch 60 - Loss: 0.155775\n",
      "[TRAIN] Epoch 61 - Loss: 0.192320\n",
      "[EVAL] Epoch 61 - Loss: 0.157597\n",
      "[TRAIN] Epoch 62 - Loss: 0.186156\n",
      "[EVAL] Epoch 62 - Loss: 0.150812\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 63 - Loss: 0.186481\n",
      "[EVAL] Epoch 63 - Loss: 0.147911\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 64 - Loss: 0.180016\n",
      "[EVAL] Epoch 64 - Loss: 0.143204\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 65 - Loss: 0.167275\n",
      "[EVAL] Epoch 65 - Loss: 0.132007\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 66 - Loss: 0.163314\n",
      "[EVAL] Epoch 66 - Loss: 0.124548\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 67 - Loss: 0.158202\n",
      "[EVAL] Epoch 67 - Loss: 0.127289\n",
      "[TRAIN] Epoch 68 - Loss: 0.158425\n",
      "[EVAL] Epoch 68 - Loss: 0.119790\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 69 - Loss: 0.160466\n",
      "[EVAL] Epoch 69 - Loss: 0.131746\n",
      "[TRAIN] Epoch 70 - Loss: 0.159167\n",
      "[EVAL] Epoch 70 - Loss: 0.110133\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 71 - Loss: 0.145593\n",
      "[EVAL] Epoch 71 - Loss: 0.107215\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 72 - Loss: 0.138549\n",
      "[EVAL] Epoch 72 - Loss: 0.101770\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 73 - Loss: 0.130976\n",
      "[EVAL] Epoch 73 - Loss: 0.097118\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 74 - Loss: 0.122577\n",
      "[EVAL] Epoch 74 - Loss: 0.082881\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 75 - Loss: 0.114476\n",
      "[EVAL] Epoch 75 - Loss: 0.067399\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 76 - Loss: 0.099960\n",
      "[EVAL] Epoch 76 - Loss: 0.058659\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 77 - Loss: 0.089159\n",
      "[EVAL] Epoch 77 - Loss: 0.052936\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 78 - Loss: 0.087370\n",
      "[EVAL] Epoch 78 - Loss: 0.052594\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 79 - Loss: 0.085007\n",
      "[EVAL] Epoch 79 - Loss: 0.047357\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 80 - Loss: 0.077368\n",
      "[EVAL] Epoch 80 - Loss: 0.036645\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 81 - Loss: 0.068811\n",
      "[EVAL] Epoch 81 - Loss: 0.024768\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 82 - Loss: 0.061153\n",
      "[EVAL] Epoch 82 - Loss: 0.027486\n",
      "[TRAIN] Epoch 83 - Loss: 0.053442\n",
      "[EVAL] Epoch 83 - Loss: 0.018957\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 84 - Loss: 0.059470\n",
      "[EVAL] Epoch 84 - Loss: 0.018582\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 85 - Loss: 0.051802\n",
      "[EVAL] Epoch 85 - Loss: 0.014404\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 86 - Loss: 0.047266\n",
      "[EVAL] Epoch 86 - Loss: 0.013800\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 87 - Loss: 0.044064\n",
      "[EVAL] Epoch 87 - Loss: 0.011658\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 88 - Loss: 0.041237\n",
      "[EVAL] Epoch 88 - Loss: 0.012737\n",
      "[TRAIN] Epoch 89 - Loss: 0.042569\n",
      "[EVAL] Epoch 89 - Loss: 0.010250\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 90 - Loss: 0.039186\n",
      "[EVAL] Epoch 90 - Loss: 0.010066\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 91 - Loss: 0.037340\n",
      "[EVAL] Epoch 91 - Loss: 0.009915\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 92 - Loss: 0.029729\n",
      "[EVAL] Epoch 92 - Loss: 0.007409\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 93 - Loss: 0.029596\n",
      "[EVAL] Epoch 93 - Loss: 0.007598\n",
      "[TRAIN] Epoch 94 - Loss: 0.030472\n",
      "[EVAL] Epoch 94 - Loss: 0.007322\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 95 - Loss: 0.026099\n",
      "[EVAL] Epoch 95 - Loss: 0.007081\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 96 - Loss: 0.029029\n",
      "[EVAL] Epoch 96 - Loss: 0.006691\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 97 - Loss: 0.027110\n",
      "[EVAL] Epoch 97 - Loss: 0.006267\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq.pt\n",
      "[TRAIN] Epoch 98 - Loss: 0.026992\n",
      "[EVAL] Epoch 98 - Loss: 0.007187\n",
      "[TRAIN] Epoch 99 - Loss: 0.026950\n",
      "[EVAL] Epoch 99 - Loss: 0.006344\n",
      "[TRAIN] Epoch 100 - Loss: 0.025282\n",
      "[EVAL] Epoch 100 - Loss: 0.006371\n",
      "Start:  2020-04-10 19:46:35.905610\n"
     ]
    }
   ],
   "source": [
    "if not MODEL_EXISTED_SEQ2SEQ or FORCE_RETRAIN:\n",
    "    encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, hidden_dim=128, z_type=1)\n",
    "    decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128)\n",
    "\n",
    "    encoder, decoder, train_losses, valid_losses = train_loop(encoder, decoder, train_x, train_y, \n",
    "                                                            validate=True, valid_x=valid_x, valid_y=valid_y, epochs=100, \n",
    "                                                            store_best_model=True, store_path=MODEL_PATH_SEQ2SEQ, store_after_epochs=50)\n",
    "else:\n",
    "    print(\"Model already existed at: %s\" % MODEL_PATH_SEQ2SEQ)\n",
    "    print(\"Training & Validating Losses also stored in the model. To force to retrain this model, please set FORCE_RETRAIN=True.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjH4H0Vuee1F"
   },
   "source": [
    "#### Train the Model: Seq2Seq + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4YhsMBkOK7OC",
    "outputId": "fb5d66ee-03b6-4320-e0d3-0456f6039f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (emb): Embedding(27, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "AttentionDecoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (attn): Attention(\n",
      "    (W): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (v): Linear(in_features=64, out_features=1, bias=False)\n",
      "  )\n",
      "  (clf): Linear(in_features=256, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n",
      "Start:  2020-04-10 19:49:44.454718\n",
      "[TRAIN] Epoch 1 - Loss: 1.052269\n",
      "[EVAL] Epoch 1 - Loss: 0.168026\n",
      "[TRAIN] Epoch 2 - Loss: 0.219793\n",
      "[EVAL] Epoch 2 - Loss: 0.024957\n",
      "[TRAIN] Epoch 3 - Loss: 0.102422\n",
      "[EVAL] Epoch 3 - Loss: 0.016096\n",
      "[TRAIN] Epoch 4 - Loss: 0.070282\n",
      "[EVAL] Epoch 4 - Loss: 0.010579\n",
      "[TRAIN] Epoch 5 - Loss: 0.058032\n",
      "[EVAL] Epoch 5 - Loss: 0.009743\n",
      "[TRAIN] Epoch 6 - Loss: 0.047720\n",
      "[EVAL] Epoch 6 - Loss: 0.006827\n",
      "[TRAIN] Epoch 7 - Loss: 0.042278\n",
      "[EVAL] Epoch 7 - Loss: 0.005948\n",
      "[TRAIN] Epoch 8 - Loss: 0.038711\n",
      "[EVAL] Epoch 8 - Loss: 0.005071\n",
      "[TRAIN] Epoch 9 - Loss: 0.038778\n",
      "[EVAL] Epoch 9 - Loss: 0.004105\n",
      "[TRAIN] Epoch 10 - Loss: 0.034048\n",
      "[EVAL] Epoch 10 - Loss: 0.005313\n",
      "[TRAIN] Epoch 11 - Loss: 0.034616\n",
      "[EVAL] Epoch 11 - Loss: 0.004636\n",
      "[TRAIN] Epoch 12 - Loss: 0.032437\n",
      "[EVAL] Epoch 12 - Loss: 0.004140\n",
      "[TRAIN] Epoch 13 - Loss: 0.027452\n",
      "[EVAL] Epoch 13 - Loss: 0.003180\n",
      "[TRAIN] Epoch 14 - Loss: 0.027727\n",
      "[EVAL] Epoch 14 - Loss: 0.002463\n",
      "[TRAIN] Epoch 15 - Loss: 0.025932\n",
      "[EVAL] Epoch 15 - Loss: 0.002385\n",
      "[TRAIN] Epoch 16 - Loss: 0.022813\n",
      "[EVAL] Epoch 16 - Loss: 0.001952\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 17 - Loss: 0.021518\n",
      "[EVAL] Epoch 17 - Loss: 0.002228\n",
      "[TRAIN] Epoch 18 - Loss: 0.022123\n",
      "[EVAL] Epoch 18 - Loss: 0.002125\n",
      "[TRAIN] Epoch 19 - Loss: 0.020911\n",
      "[EVAL] Epoch 19 - Loss: 0.001957\n",
      "[TRAIN] Epoch 20 - Loss: 0.019123\n",
      "[EVAL] Epoch 20 - Loss: 0.001678\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 21 - Loss: 0.020029\n",
      "[EVAL] Epoch 21 - Loss: 0.001599\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 22 - Loss: 0.019860\n",
      "[EVAL] Epoch 22 - Loss: 0.001746\n",
      "[TRAIN] Epoch 23 - Loss: 0.018512\n",
      "[EVAL] Epoch 23 - Loss: 0.001445\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 24 - Loss: 0.016851\n",
      "[EVAL] Epoch 24 - Loss: 0.001457\n",
      "[TRAIN] Epoch 25 - Loss: 0.017845\n",
      "[EVAL] Epoch 25 - Loss: 0.001555\n",
      "[TRAIN] Epoch 26 - Loss: 0.018350\n",
      "[EVAL] Epoch 26 - Loss: 0.001720\n",
      "[TRAIN] Epoch 27 - Loss: 0.017702\n",
      "[EVAL] Epoch 27 - Loss: 0.001421\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 28 - Loss: 0.017490\n",
      "[EVAL] Epoch 28 - Loss: 0.001397\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 29 - Loss: 0.017148\n",
      "[EVAL] Epoch 29 - Loss: 0.001392\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 30 - Loss: 0.016750\n",
      "[EVAL] Epoch 30 - Loss: 0.001306\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 31 - Loss: 0.015714\n",
      "[EVAL] Epoch 31 - Loss: 0.001227\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 32 - Loss: 0.017157\n",
      "[EVAL] Epoch 32 - Loss: 0.001136\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 33 - Loss: 0.014273\n",
      "[EVAL] Epoch 33 - Loss: 0.001145\n",
      "[TRAIN] Epoch 34 - Loss: 0.015525\n",
      "[EVAL] Epoch 34 - Loss: 0.001046\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 35 - Loss: 0.015789\n",
      "[EVAL] Epoch 35 - Loss: 0.001104\n",
      "[TRAIN] Epoch 36 - Loss: 0.017716\n",
      "[EVAL] Epoch 36 - Loss: 0.000953\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 37 - Loss: 0.015709\n",
      "[EVAL] Epoch 37 - Loss: 0.000932\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 38 - Loss: 0.015744\n",
      "[EVAL] Epoch 38 - Loss: 0.000815\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 39 - Loss: 0.016577\n",
      "[EVAL] Epoch 39 - Loss: 0.000986\n",
      "[TRAIN] Epoch 40 - Loss: 0.015109\n",
      "[EVAL] Epoch 40 - Loss: 0.001018\n",
      "[TRAIN] Epoch 41 - Loss: 0.015969\n",
      "[EVAL] Epoch 41 - Loss: 0.000946\n",
      "[TRAIN] Epoch 42 - Loss: 0.015850\n",
      "[EVAL] Epoch 42 - Loss: 0.001094\n",
      "[TRAIN] Epoch 43 - Loss: 0.013866\n",
      "[EVAL] Epoch 43 - Loss: 0.000780\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 44 - Loss: 0.015347\n",
      "[EVAL] Epoch 44 - Loss: 0.001069\n",
      "[TRAIN] Epoch 45 - Loss: 0.013756\n",
      "[EVAL] Epoch 45 - Loss: 0.000820\n",
      "[TRAIN] Epoch 46 - Loss: 0.014692\n",
      "[EVAL] Epoch 46 - Loss: 0.000748\n",
      "[*] Stored best model to /content/drive/My Drive/Colab Notebooks/data/NLP/HW3/output/seq2seq+attn.pt\n",
      "[TRAIN] Epoch 47 - Loss: 0.013485\n",
      "[EVAL] Epoch 47 - Loss: 0.001002\n",
      "[TRAIN] Epoch 48 - Loss: 0.014909\n",
      "[EVAL] Epoch 48 - Loss: 0.000754\n",
      "[TRAIN] Epoch 49 - Loss: 0.014411\n",
      "[EVAL] Epoch 49 - Loss: 0.000847\n",
      "[TRAIN] Epoch 50 - Loss: 0.013821\n",
      "[EVAL] Epoch 50 - Loss: 0.000955\n",
      "Start:  2020-04-10 19:58:05.367062\n"
     ]
    }
   ],
   "source": [
    "if not MODEL_EXISTED_SEQ2SEQ_ATTENTION or FORCE_RETRAIN:\n",
    "    encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, hidden_dim=128, z_type=0)\n",
    "    decoder = AttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128, attn_size=64)\n",
    "\n",
    "    encoder, decoder, train_losses, valid_losses = train_loop(encoder, decoder, train_x, train_y, \n",
    "                                                            validate=True, valid_x=valid_x, valid_y=valid_y, epochs=50, \n",
    "                                                            store_best_model=True, store_path=MODEL_PATH_SEQ2SEQ_ATTENTION, \n",
    "                                                            store_after_epochs=15)\n",
    "else:\n",
    "    print(\"Model already existed at: %s\" % MODEL_PATH_SEQ2SEQ_ATTENTION)\n",
    "    print(\"Training & Validating Losses also stored in the model. To force to retrain this model, please set FORCE_RETRAIN=True.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRUR57HyOy_h"
   },
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6oPdilVlNFv"
   },
   "source": [
    "#### Predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfpVP4tIOy_i"
   },
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, samples, index_to_elem, remove_ending=True):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    preds = []\n",
    "    for i in range(len(samples)):\n",
    "        encoded = encoder([samples[i]])\n",
    "        pred = decoder.predict(encoded, START_IX, STOP_IX, maxlen=15)\n",
    "        pred = [index_to_elem[ix] for ix in pred]\n",
    "        if remove_ending:\n",
    "            preds.append(''.join(pred).replace('<end>', ''))\n",
    "        else:\n",
    "            preds.append(''.join(pred))\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDxXOt_1ePlT"
   },
   "source": [
    "#### With validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUzc9LnberC2"
   },
   "source": [
    "##### **Vanilla Seq2Seq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "k3jGnlrrOy_k",
    "outputId": "1ebafe79-ba8f-44f2-cc2c-c79d64f4cad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.998\n",
      "Examples:\n",
      "+ jjuvvmmmauuu --> ajmuv --> ajmuv\n",
      "+ ceeeppnnn --> cenp --> cenp\n",
      "+ iiikkxxaaijjjnnaaeee --> aeijknx --> aeijknx\n",
      "+ hqqtttnnnooawwc --> achnoqtw --> achnoqtw\n",
      "+ zzooeeeooonnzkhjhhh --> ehjknoz --> ehjknoz\n",
      "+ aaaepppiitmbbb --> abeimpt --> abeimpt\n",
      "+ wssrriidyyyf --> dfirswy --> dfirswy\n",
      "+ kkuhhcrrzzmmmwww --> chkmruwz --> chkmruwz\n",
      "+ cssxx --> csx --> csx\n",
      "+ ddooouulllelllc --> cdelou --> cdelou\n",
      "+ lllkkkaaaggfihrrr --> afghiklr --> afghiklr\n",
      "+ ttgbbiillzocccuuu --> bcgilotuz --> bcgilotuz\n",
      "+ lliiieeexiivvvtttrrrfffxx --> efilrtvx --> efilrtvx\n",
      "+ parhhhylll --> ahlpry --> ahlpry\n",
      "+ qqqccciialrrrvvvlzzz --> acilqrvz --> acilqrvz\n",
      "+ cczppkq --> ckpqz --> ckpqz\n",
      "+ ffffffsaa --> afs --> afs\n",
      "+ kooopppwooeet --> ekoptw --> ekoptw\n",
      "+ oyyyiiio --> ioy --> ioy\n",
      "+ nnnyyxxuuufff --> fnuxy --> fnuxy\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, hidden_dim=128, z_type=1)\n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128)\n",
    "\n",
    "state = torch.load(MODEL_PATH_SEQ2SEQ)\n",
    "encoder.load_state_dict(state['encoder'])\n",
    "decoder.load_state_dict(state['decoder'])\n",
    "\n",
    "predictions = predict(encoder, decoder, valid_x, tgt_vocab.itos, remove_ending=False)\n",
    "groundtruth = [''.join(t) for t in valid_out]\n",
    "\n",
    "print('Accuracy: %s' % accuracy_score(groundtruth, predictions))\n",
    "print('Examples:')\n",
    "for i in range(len(valid_inp[:20])):\n",
    "    x = ''.join(valid_inp[i])\n",
    "    y = ''.join(valid_out[i])\n",
    "    \n",
    "    print(f\"+ {x} --> {y} --> {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "d57lWAydOGfx",
    "outputId": "332e1487-accf-459b-8deb-5122101253ef"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = torch.load(MODEL_PATH_SEQ2SEQ_LOSS)\n",
    "\n",
    "losses = state['losses']\n",
    "train_losses = losses['train_losses']\n",
    "valid_losses = losses['valid_losses']\n",
    "\n",
    "plt.plot(range(len(train_losses)), train_losses)\n",
    "plt.plot(range(len(train_losses)), valid_losses)\n",
    "plt.title('Figure 1: Train Loss and Evaluting Loss of Vanilla Seq2Seq Model')\n",
    "plt.legend(['Train Loss', 'Eval Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ke5gIwee3ez"
   },
   "source": [
    "##### **Seq2Seq + Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "IUWpFh7kOy_t",
    "outputId": "b6920a86-b87d-4839-fd5a-2b6beea3b923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9996\n",
      "Examples:\n",
      "+ jjuvvmmmauuu --> ajmuv --> ajmuv\n",
      "+ ceeeppnnn --> cenp --> cenp\n",
      "+ iiikkxxaaijjjnnaaeee --> aeijknx --> aeijknx\n",
      "+ hqqtttnnnooawwc --> achnoqtw --> achnoqtw\n",
      "+ zzooeeeooonnzkhjhhh --> ehjknoz --> ehjknoz\n",
      "+ aaaepppiitmbbb --> abeimpt --> abeimpt\n",
      "+ wssrriidyyyf --> dfirswy --> dfirswy\n",
      "+ kkuhhcrrzzmmmwww --> chkmruwz --> chkmruwz\n",
      "+ cssxx --> csx --> csx\n",
      "+ ddooouulllelllc --> cdelou --> cdelou\n",
      "+ lllkkkaaaggfihrrr --> afghiklr --> afghiklr\n",
      "+ ttgbbiillzocccuuu --> bcgilotuz --> bcgilotuz\n",
      "+ lliiieeexiivvvtttrrrfffxx --> efilrtvx --> efilrtvx\n",
      "+ parhhhylll --> ahlpry --> ahlpry\n",
      "+ qqqccciialrrrvvvlzzz --> acilqrvz --> acilqrvz\n",
      "+ cczppkq --> ckpqz --> ckpqz\n",
      "+ ffffffsaa --> afs --> afs\n",
      "+ kooopppwooeet --> ekoptw --> ekoptw\n",
      "+ oyyyiiio --> ioy --> ioy\n",
      "+ nnnyyxxuuufff --> fnuxy --> fnuxy\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, hidden_dim=128, z_type=0)\n",
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128, attn_size=64)\n",
    "\n",
    "state = torch.load(MODEL_PATH_SEQ2SEQ_ATTENTION)\n",
    "encoder.load_state_dict(state['encoder'])\n",
    "decoder.load_state_dict(state['decoder'])\n",
    "\n",
    "predictions = predict(encoder, decoder, valid_x, tgt_vocab.itos, remove_ending=False)\n",
    "groundtruth = [''.join(t) for t in valid_out]\n",
    "\n",
    "print('Accuracy: %s' % accuracy_score(groundtruth, predictions))\n",
    "print('Examples:')\n",
    "for i in range(len(valid_inp[:20])):\n",
    "    x = ''.join(valid_inp[i])\n",
    "    y = ''.join(valid_out[i])\n",
    "    \n",
    "    print(f\"+ {x} --> {y} --> {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "3CJ30dsFSxES",
    "outputId": "905aa803-0119-4c79-e423-8386524ca975"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAEICAYAAADocntXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZhcVbnv8e9bU08ZIAMB0oEEjUAIIdFAgABGhiuTwkFRFCG5KBw9Kt6LiuAEcg734HCPOHAu4IQiinhUDkoQB0ABRQiKIJOEEJLGABnI2Omp6r1/rNXdu4vqIUmnuir1+zxPPbWnWnvttXftd6+1V9U2d0dERKSapEY6AyIiIttKwUtERKqOgpeIiFQdBS8REak6Cl4iIlJ1FLxERKTqbHfwMrPNZrbfcGammpnZHWa2cKTzMVLMbIGZtYzAepeb2fHb+dl94nGcHu58VSoz+yczWxm3e85I50e2n5mdbWa/Gul8DJWZXW5m3x/isveY2fsGWmbQ4BVPDlvjwd792tvdR7n7sqFmfGcxs1PM7D4zW29mL5rZN81s9BA/m9ymQtF2nr0t+XD3k9z9u9u5Ddt9Aq4WZuZmtqWozC8ucx76lLO7r4jHcX4nrOsGM/u34U53GHwJ+FDc7r8UzzSz08zsETPbaGZrzOwuM5u2oys1s4Vm9nBMt8XMvmBmmcT8o8zsD2a2wczWmdn9Znbojq53uMUTsJvZvKLpr9rfw/m9NrOpcb09ZebuN7n7/xiO9IvWtSCu62dF0w+J0+8Z7nVuj6HWvN4SD/bu1z92VoaSO2eIxgL/BuwNHAhMBr44lA8mtwlYQd/tvGkH8iSlHVJ0HH1hpDNUg/YFHi81w8xeC3wP+CjhezUNuAYYjuDeCPwvYAIwDzgO+Fhc7xjgF8DXgHGE7/DngPZhWG+/YkBYvg3LG3AusC6+78pWA0eY2fjEtIXA30coP6/m7gO+gOXA8SWmO/DaODwe+DmwEXiIEEzui/OmxmUzic/eA7wvDi8C7ge+DKyNn60jXCGuAF4CrgUaBstrTO8M4LGhLNvfdgILgBbgE8CLwI3A7oQv2GrglTjcPMA23Re34RXgOeCk7SjjOuBq4B/xdTVQF+dNiHlYT/gy3Quk4rxPAC8Am4CngeP6We8pwF/iflsJXJ6Y173fFsb9sAb4VGJ+A3BD3L4ngI8DLQNsY8/xUjR9b2ArMC4xbU5cXxZ4DXBXPDbWADcBu/Wz324A/i0xb0F3nuI+LMR1bQYupujYjPvwXwnH4ybgV8CERHrnAs/HvHymv/1WKi9F884Hlsb9dhuwd5xuhO/By3GfPAbMjPNOjuW8Ke7bj/WTdgr4dMzny4RgNDYeS5vj9m4Bni3x2bcDjwywD1PAJcCzsQxuKdpv5yTK51ODlM9FwM/j8Fxg/SDfz/OAJ+Pxdiewb2LeCcBTwAbg68DviN/FQdKcCizfhnPEMfH4OTtuYy5OvwDoBDpiGf+81PEWlz0c+APhe/tXYEHROaTk8Uf4DnpMazNwBPE8k/j8kYTz74b4fuRQ0i6xnQsI579rgQ/Gael43H0WuGeI65wW98Um4Ndx33w/MX+wshhwHw5X8Lo5vhqBGYQT4bYEry7gw0CGcFL8MuFLPQ4YHQ+Gfx/iAXY1cHNi/BLgF9uynXHndQGfJ3zpGwgB+m1xG0cDPwZuHWCbOgknqTTwAULwsW0s4yuAB4A9gIlxR/9rnPfv8eDKxtfRhJPf/rH8u0+IU4HXDHCQHkw4Kc0iXCicXrTfvhG3/xDClfCBcf5VhIA5DpgC/I3tCF5x3l3A+YnxLwLXxuHXEk5OdbEMfg9c3c9+u4F+glepcqZ08HoWeF3c5nuAq+K8GYSTxlFAjnBh0llqv5XKS2L6sYQg/Pq4TV8Dfh/nvRl4GNgt7ssDgb3ivFXA0XF4d+D1/az3PEJg3A8YBfwUuHGI+2E/oI3w/XsTMKpo/kcIx2NzzPt1wA+LyueYOO8/CN+h/srn1kTZjiEEg+8CJwG7Fy17WtymAwnniE8Df4jzJhBOjm8nfA/+d1zvzghe3yIE7GzM79sG2t8ljrfJ8XMnE75zJ8TxiUM4/qby6vPoInrPs+MIgf2cWEbviuPjB0u7n/NCCyEw/SlOO5lw0fA+YvAawjr/GI+DunhcbCIGryGWxbAEr82E6LieeMKOBflawsm5E9g/8ZltrXmtSMwzwpXhaxLTjgCeG0JeT4iF97qhHpClDrS48zqA+gGWnw28MsA2LU3Ma4xlsOdg6y6a/ixwcmL8zcQvGyGw/TdFJ6K4T14Gjgey21gGVwNfLtpvydrlg8BZcXgZcGJi3gUMHrw2Jo6j9cCb47z3AXcl9v9K4Jh+0jkd+Es/++0Gdjx4fTox/1+AX8bhzxJP1Il92lFqv5XKS2L6t4AvJMZHEb4/UwmB7e+EK9JU0edWAP8MjBlkH/4W+JfE+P4x/e5t7Dd4xfmHE07QqwmB7AZiECPUfI5LLLtXd9qxfJIXjU39lQ8hwLbQt1Z7YFxXCyH43AZMivPuAN6bWDYFtBKaQM8FHkjMs5jGsAavuL830ntxdx3w3wPt7xLH2ydIXEjEaXcCC4dw/PU5VuO0RfSeZ88BHixK+4/AosHSLrGtC+htsXgmHkM3E2qcyeDV7zqBfeJ+bErM+wG9wWsoZTHgPhzqPa/T3X23+Dq9aN5EwsG7MjFtJdsmufxEwoHycOyEsR74ZZzeLzM7nFA4b3f34WiXXe3ubYn0G83sOjN73sw2EmoAuw3QU+3F7gF3b42Do7YxD3sTmmG6PR+nQaidLAV+ZWbLzOySuK6lhHsLlwMvm9nNZrY3JZjZPDO728xWm9kG4P2EK9mS20E4YXRvw9703W/JfPbn9YnjaDd3vzNO/wmhfX0vwhVagVCrw8wmxW14IZb790vkcTgNaXvjPl27Hen32afuvjmmM9nd7yI0rVxD2HfXx/tBEGr9JwPPm9nvzOyIoaQfhzPApKFkzt0fcPd3uPtEQm3+GEITIIRg8bPE9/JJwv2wSby6fLZQonzM7HRCq8FJ7r4msfyT7r7I3ZuBmTG9qxPr/UpivesIQWpyifU6A5x/zOzdiXQeBfbpHo+vffr56D8RTsaL4/hNwElmNuB5qci+wJnJ9RFq8nsllunv+BtM8X4njk/ewbRvBD5EqIn/rGjeQOvcm3Bxv6VoXrehlMWAhuN3XqsJO7U5MW1KYrg7842JaXsWpeGJ4TWEduKDEie5sR46VZRkocvvbcB57v7bbd2AfnjR+EcJVyDz3H0M4UsN4Uu0s/yDsJO77ROn4e6b3P2j7r4f8FbgIjM7Ls77gbsfFT/rhObPUn5AKLcp7j6W0Aw51O1ZRd/93N+XflDu/gqhDf6dwLsJV/Dd5f9/CNtwcCz39wyQxy0M/TjbVqtIHONm1t2UvK367FMza4rpvADg7l919zcQmuFeR7iXiLs/5O6nEZqQbyXUjgZNn94r4Je2NaPu/hCh2XFmnLSSEHSSFyD17v4CRceDmTVSVD5mdiKhGfot7v7YAOt9ilCTSa73n4vW2+DufyixXqPvcVmc9g+60yA0la8oSndFPx9dSDjZrzCzFwm3DbKE4xVKH1vF01YSahvJ9TW5+1X95XeAtIoV73cI+/6FIaQ9kBsJtbTFiYvwoaxzFbB7PL6T87rtSFkAwxC8PHQz/ilweaydHECiJ467ryZszHvMLG1m5xFuwveXXoFwgH/ZzPYAMLPJZvbmUsub2UxCzezD7v7zHd2eAYwmBNX1ZjYOuGyY08+aWX3ilQF+CHzazCaa2QRC08z3AczsVDN7bfyybiBcARfMbH8zO9bM6gjNPlsJNZn+tmmdu7eZ2WH0fhGH4hbgUjPb3cyaCfcsd8QPCMfN2+NwMo+bgQ1mNpl4Mu/HI8DJZjbOzPYk1ECTXiLc19ke/wW8xcyONLMcoWY7WKBPF+3THGGf/k8zmx330f8h3FdYbmaHxtpwlhCI2wj7NGfhNz1j3b2T0HzV3z79IfC/zWyamY2K6f/I3bsG20AL3dXPT3zvDiBcGD0QF7kWuNLM9o3zJ5rZaYnyOTWmkSM0a6cSaR9LqK28zd0fLFrvAWb20XgcYWZTCPdPkuu91MwOivPHmtmZcd7twEFmdkb8zlzIqy9adkg87o4DTiXcLphNuAf8eXrPdaWOreJp3yccQ2+O58J6C93SmxncasI+7+/4XQy8LtYsM2b2TsIF0C+GkHa/3P054I301r6HtE53fx5YAnwuHr9HAW9JfHZHygIYvn/Y+BChR1N3z7wf0reb6/mEk85a4CBCx4OBfILQJPZAbCr6DaHWU8pHCU2K37Le3w/1dAU2s0+a2R3bvkmvcjXhRucawpfql8OQZtJiQqDpfl1OuHe4hNC88Rjw5zgNYDqhXDYT2pn/093vJtwcvSrm80XClfql/azzX4ArzGwTITD2dzVfyucIzQDPEWpNNw7hM3+1vr/zujox77a4TS+6+1+L1vN6QoC+nXCh1J8bCb2Wlsc8/aho/r8TLgbWm9nHhpDfHu7+OCFA30y4qtxMuLc4UHfuS+i7T+9y998Qeir+JKbzGuCsuPwYwoXbK/T22uv+2cc5wPL4fXg/4f5DKd8mlMPvCfumjaFfWKwnBKvHzGwz4Rj/GdD9k4avEPbTr+Ix8wCh23t3+XyQcOGxKm5D8kfrnyGcIxYn9n/393JTTOdPZrYlpvs3wncbd/8ZIVDcHLf/b4SOHcSmxzMJx/xawjF0/xC3d6jOIfTC/JW7v9j9Ar4KzIoX0N8CZsRj69b4uT7Hm7uvJHQ++SQhGK0knBcHPQ/HWs+VwP0xvcOL5q8lBNePEsrhYuDUZNPs9nL3+7zEz6OGsM53E/brOsLF/vcSn93usuhmva0zw8fMPk/onLBw2BMXqQCxVrMemB6vTqWIhd9QvS8G7HKu9x5Cx4BvlnO9Ul7DUvOK1f5ZFhwGvJdX39wTqWpm9pbYNN5E6Cr/GKGWJyJlNlzNhqMJzTlbCE01/5fQjVtkV3IavT8Yn0742cDwN12IyKB2SrOhiIjIzqRHooiISNXZJf5wdsKECT516tSRzoaISNV4+OGH18Qfo1elXSJ4TZ06lSVLlox0NkREqoaZDeVfcSqWmg1FRKTqKHiJiEjVUfASEZGqs0vc8xKRXUdnZyctLS20tbUNvrAMqr6+nubmZrLZ7EhnZVgpeIlIRWlpaWH06NFMnTqV8L/Tsr3cnbVr19LS0sK0adNGOjvDSs2GIlJR2traGD9+vALXMDAzxo8fv0vWYhW8RKTiKHANn121LGs2eLk7X/7137l/6Q4/MUBERMqsZoOXmfH/7nmW+xS8RCRh7dq1zJ49m9mzZ7PnnnsyefLknvGOjo4BP7tkyRIuvPDCbVrf1KlTWbNG56FtVdMdNhpyaVrbB33ArIjUkPHjx/PII48AcPnllzNq1Cg+9rHeZ5d2dXWRyZQ+dc6dO5e5c+eWJZ+1rmZrXgCNuTStHfmRzoaIVLhFixbx/ve/n3nz5nHxxRfz4IMPcsQRRzBnzhyOPPJInn76aQDuueceTj31VCAEvvPOO48FCxaw33778dWvfnXI61u+fDnHHnsss2bN4rjjjmPFihUA/PjHP2bmzJkccsghHHPMMQA8/vjjHHbYYcyePZtZs2bxzDPPDPPWVybVvDoVvEQq1ed+/jhP/GPjsKY5Y+8xXPaWg7b5cy0tLfzhD38gnU6zceNG7r33XjKZDL/5zW/45Cc/yU9+8pNXfeapp57i7rvvZtOmTey///584AMfGNLvrT784Q+zcOFCFi5cyLe//W0uvPBCbr31Vq644gruvPNOJk+ezPr16wG49tpr+chHPsLZZ59NR0cH+XxtnNNqOng15tJsVc1LRIbgzDPPJJ1OA7BhwwYWLlzIM888g5nR2dlZ8jOnnHIKdXV11NXVsccee/DSSy/R3Nw86Lr++Mc/8tOf/hSAc845h4svvhiA+fPns2jRIt7xjndwxhlnAHDEEUdw5ZVX0tLSwhlnnMH06dOHY3MrXm0Hr2yG1g7d8xKpVNtTQ9pZmpqaeoY/85nP8KY3vYmf/exnLF++nAULFpT8TF1dXc9wOp2mq2vHzjfXXnstf/rTn7j99tt5wxvewMMPP8y73/1u5s2bx+23387JJ5/Mddddx7HHHrtD66kGNX3Pq0E1LxHZDhs2bGDy5MkA3HDDDcOe/pFHHsnNN98MwE033cTRRx8NwLPPPsu8efO44oormDhxIitXrmTZsmXst99+XHjhhZx22mk8+uijw56fSlTW4GVm3zazl83sb/3MNzP7qpktNbNHzez1OzM/6rAhItvj4osv5tJLL2XOnDk7XJsCmDVrFs3NzTQ3N3PRRRfxta99je985zvMmjWLG2+8ka985SsAfPzjH+fggw9m5syZHHnkkRxyyCHccsstzJw5k9mzZ/O3v/2Nc889d4fzUw3M3cu3MrNjgM3A99x9Zon5JwMfBk4G5gFfcfd5g6U7d+5c356HUV50yyP8adk67r9k169ii1SLJ598kgMPPHCks7FLKVWmZvawu1dtv/6y1rzc/ffAugEWOY0Q2NzdHwB2M7O9dlZ+GnNptqq3oYhI1am0e16TgZWJ8ZY47VXM7AIzW2JmS1avXr1dK2vMqcOGiEg1qrTgNWTufr27z3X3uRMnTtyuNBqyado6CxQK5Ws6FRGRHVdpwesFYEpivDlO2ykac+E3G2o6FBGpLpUWvG4Dzo29Dg8HNrj7qp21ssa68DM39TgUEakuZf2Rspn9EFgATDCzFuAyIAvg7tcCiwk9DZcCrcD/3Jn5aczGmpeCl4hIVSl3b8N3ufte7p5192Z3/5a7XxsDF7GX4Qfd/TXufrC7b3v/923Q3WzY2qlOGyLSK51O9zwGZfbs2Vx11VXblc6CBQso9TOe/qbL0NX030M1xOC1pV01LxHp1dDQ0PNYFKlMlXbPq6wacyF2q9lQRAbzy1/+kjPPPLNnPPn4kw984APMnTuXgw46iMsuu2y70l+3bh2nn346s2bN4vDDD+/5m6ff/e53PTXAOXPmsGnTJlatWsUxxxzD7NmzmTlzJvfee++Ob2CVqemaV0+zoX7rJVKZ7rgEXnxseNPc82A4aeBmwK1btzJ79uye8UsvvZS3ve1tXHDBBWzZsoWmpiZ+9KMfcdZZZwFw5ZVXMm7cOPL5PMcddxyPPvoos2bN2qZsXXbZZcyZM4dbb72Vu+66i3PPPZdHHnmEL33pS1xzzTXMnz+fzZs3U19fz/XXX8+b3/xmPvWpT5HP52ltbd32cqhyNR28GtRVXkRK6K/Z8MQTT+TnP/85b3/727n99tv5whe+AMAtt9zC9ddfT1dXF6tWreKJJ57Y5uB133339TwT7Nhjj2Xt2rVs3LiR+fPnc9FFF3H22Wdzxhln0NzczKGHHsp5551HZ2cnp59+ep9AWytqOnj11rwUvEQq0iA1pHI766yz+PrXv864ceOYO3cuo0eP5rnnnuNLX/oSDz30ELvvvjuLFi2ira1t2NZ5ySWXcMopp7B48WLmz5/PnXfeyTHHHMPvf/97br/9dhYtWsRFF11UM3/I262273ll9TsvERm6N77xjfz5z3/mG9/4Rk+T4caNG2lqamLs2LG89NJL3HHHHduV9tFHH81NN90EhPtpEyZMYMyYMTz77LMcfPDBfOITn+DQQw/lqaee4vnnn2fSpEmcf/75vO997+PPf/7zsG1jtajpmldPs6HueYlIQvE9rxNPPJGrrrqKdDrNqaeeyg033MB3v/tdAA455BDmzJnDAQccwJQpU5g/f/6Q1nHKKaeQzWaB8DTk6667jvPOO49Zs2bR2NjYk/7VV1/N3XffTSqV4qCDDuKkk07i5ptv5otf/CLZbJZRo0bxve99b5hLoPKV9ZEoO8v2PhIF4LWfXMwFx+zHxSceMMy5EpHtoUeiDD89EmUX1KAHUoqIVJ2aD16NubR+5yUiUmUUvHIZWtVVXqSi7Aq3MyrFrlqWNR+8GrJpddgQqSD19fWsXbt2lz3plpO7s3btWurr60c6K8OupnsbQmg21D0vkcrR3NxMS0sL2/uEdOmrvr6e5ubmkc7GsFPwqsuwcWvnSGdDRKJsNsu0adNGOhtS4Wq+2bAxqw4bIiLVRsErl2aL7nmJiFSVmg9eDeoqLyJSdWo+eKnDhohI9an54NWQy7C1M0+hoG65IiLVouaDV/djUdq6VPsSEakWCl56ppeISNWp+eDVkO1+LIqCl4hItaj54NWY0wMpRUSqjYJXT7OhfuslIlItaj549T5NWTUvEZFqUfPBSx02RESqj4JXd/DSM71ERKpGzQevhthhQ8/0EhGpHjUfvBqzajYUEak2ZQ1eZnaimT1tZkvN7JIS8/cxs7vN7C9m9qiZnbyz89Sge14iIlWnbMHLzNLANcBJwAzgXWY2o2ixTwO3uPsc4CzgP3d2vuoyKdIpU29DEZEqUs6a12HAUndf5u4dwM3AaUXLODAmDo8F/rGzM2VmNGb1TC8RkWpSzuA1GViZGG+J05IuB95jZi3AYuDD/SVmZheY2RIzW7J69eodypie6SUiUl0qrcPGu4Ab3L0ZOBm40cxK5tHdr3f3ue4+d+LEiTu0Uj3TS0SkupQzeL0ATEmMN8dpSe8FbgFw9z8C9cCEnZ2xhlxGwUtEpIqUM3g9BEw3s2lmliN0yLitaJkVwHEAZnYgIXjtWJvgEDTm0mzt1D0vEZFqUbbg5e5dwIeAO4EnCb0KHzezK8zsrXGxjwLnm9lfgR8Ci9x9pz/iWM2GIiLVJVPOlbn7YkJHjOS0zyaGnwDmlzNPEJ7ptXpTe7lXKyIi26nSOmyMCNW8RESqi4IX6rAhIlJtFLyIHTb0I2URkaqh4EVsNuzMU4a+ISIiMgwUvAj/sOEO7V2Fkc6KiIgMgYIXeiyKiEi1UfACGuMDKVt130tEpCooeNH7TC/9Oa+ISHVQ8CJ02ADYouAlIlIVFLxQs6GISLVR8KK35qVmQxGR6qDgRW/wUm9DEZHqoOCFOmyIiFQbBS90z0tEpNooeJFoNuxUzUtEpBooeAF1mRRmajYUEakWCl6AmdGY1TO9RESqhYJXpGd6iYhUDwWvSM/0EhGpHgpeUWNOzYYiItVCwStqyKXZqt6GIiJVQcErUs1LRKR6KHhFDVl12BARqRYKXlGoeanDhohINVDwitRsKCJSPRS8ooZcWv+wISJSJRS8oqZchtaOLtx9pLMiIiKDUPCKGnJpCg7tXYWRzoqIiAyirMHLzE40s6fNbKmZXdLPMu8wsyfM7HEz+0G58qanKYuIVI9MuVZkZmngGuAEoAV4yMxuc/cnEstMBy4F5rv7K2a2R7nyl3wsyu7lWqmIiGyXcta8DgOWuvsyd+8AbgZOK1rmfOAad38FwN1fLlfmGuIDKfX/hiIila+cwWsysDIx3hKnJb0OeJ2Z3W9mD5jZif0lZmYXmNkSM1uyevXqHc5cYzbWvNRsKCJS8Sqtw0YGmA4sAN4FfMPMdiu1oLtf7+5z3X3uxIkTd3jFPc2GCl4iIhWvnMHrBWBKYrw5TktqAW5z9053fw74OyGY7XQN6rAhIlI1yhm8HgKmm9k0M8sBZwG3FS1zK6HWhZlNIDQjLitH5hrjPS/VvEREKl/Zgpe7dwEfAu4EngRucffHzewKM3trXOxOYK2ZPQHcDXzc3deWI3+9zYbqsCEiUunK1lUewN0XA4uLpn02MezARfFVVj3Nhnqml4hIxau0DhsjRh02RESqh4JXVJ+JwatdzYYiIpVOwStKpYyGrB6LIiJSDRS8EhpzaVp1z0tEpOIpeCXomV4iItVBwSshPE1Z97xERCqdgldCYy6je14iIlVAwSuhUc2GIiJVQcErITQbKniJiFQ6Ba+EhlxG/7AhIlIFFLwSGrPqsCEiUg0UvBIa1GwoIlIVFLwS1GFDRKQ6KHglNObSdBWcjq7CSGdFREQGoOCV0BAfSKnal4hIZVPwSuh5LEqnOm2IiFQyBa+E7uC1pV01LxGRSqbgldCQjU9TVrOhiEhFU/BKaIz3vPRbLxGRyqbgldDQc89LNS8RkUqm4JXQfc9LzYYiIpVNwSuhp7ehgpeISEVT8Epo6Kl56Z6XiEglU/BKaOrpsKGal4hIJVPwSujuKq/gJSJS2RS8ElIpoz6b0jO9REQqnIJXkcZcRr/zEhGpcApeRRqyeqaXiEilU/Aqomd6iYhUvrIGLzM70cyeNrOlZnbJAMu9zczczOaWM38QgpdqXiIila1swcvM0sA1wEnADOBdZjajxHKjgY8AfypX3pIaVPMSEal45ax5HQYsdfdl7t4B3AycVmK5fwU+D7SVMW89GnMZtqjDhohIRStn8JoMrEyMt8RpPczs9cAUd799sMTM7AIzW2JmS1avXj1smVTNS0Sk8lVMhw0zSwH/AXx0KMu7+/XuPtfd506cOHHY8tGo3oYiIhWvnMHrBWBKYrw5Tus2GpgJ3GNmy4HDgdvK3WkjdNhQs6GISCUrZ/B6CJhuZtPMLAecBdzWPdPdN7j7BHef6u5TgQeAt7r7kjLmkYZcRv+wISJS4coWvNy9C/gQcCfwJHCLuz9uZleY2VvLlY/BNObSdOadznxhpLMiIiL9yJRzZe6+GFhcNO2z/Sy7oBx5KpZ8ptfYhoq5JSgiIgk6Oxdp0NOURUQqnoJXkd5neqnThohIpVLwKtKQ0zO9REQqnYJXke57XupxKCJSuRS8ijSq5iUiUvEUvIo0ZMM9r6265yUiUrEUvIqo5iUiUvkUvIooeImIVD4FryK9vQ3VbCgiUqkUvIo09vzOSzUvEZFKpeBVJJ0ycpmU/mFDRKSCKXiVEB6LouAlIlKpFLxK0AMpRUQqm4JXCQ25NFs71WFDRKRSKXiV0JjLqOYlIlLBFLxKaNA9LxGRiqbgVUJjLq3ehiIiFUzBq4TQ21D3vEREKpWCVwmNuYxqXiIiFUzBq4TGXJpWPc9LRKRiKXiVoA4bIiKVTcGrhMZsho6uAvmCj3RWRESkBAWvEhr1z/IiIhVNwauE7seiqNOGiEhlUlvT4PQAABIGSURBVPAqobvmtUXBS0SkIil4laBmQxGRyqbgVUJDfCClmg1FRCqTglcJTbHmtbGtc4RzIiIipZQ1eJnZiWb2tJktNbNLSsy/yMyeMLNHzey3ZrZvOfPX7YC9xpBLp/jD0rUjsXoRERlE2YKXmaWBa4CTgBnAu8xsRtFifwHmuvss4L+AL5Qrf0mj6jIc8Zrx/PrJl3DXb71ERCpNOWtehwFL3X2Zu3cANwOnJRdw97vdvTWOPgA0lzF/fZwwYxLPr21l6cubRyoLIiLSj3IGr8nAysR4S5zWn/cCd/Q308wuMLMlZrZk9erVw5TFXscfOAmAXz/50rCnLSIiO6YiO2yY2XuAucAX+1vG3a9397nuPnfixInDnoc9x9Yzq3ksv35CwUtEpNKUM3i9AExJjDfHaX2Y2fHAp4C3unt7mfJW0vEHTuKRlet5eVPbSGZDRESKlDN4PQRMN7NpZpYDzgJuSy5gZnOA6wiB6+Uy5q2kE2ZMwh3uenLEsyIiIgllC17u3gV8CLgTeBK4xd0fN7MrzOytcbEvAqOAH5vZI2Z2Wz/JlcUBe45m8m4N/Eb3vUREKkqmnCtz98XA4qJpn00MH1+2zOS7oOVBaJoIE6aXXMTMOGHGJH744ApaO7pozJW1uEREpB8V2WGjLApd8L3T4eEbBlzshBmTaO8qcN8za8qTLxERGVTtBq9sPUw5DJbfO+Bih00bx+j6jHodiohUkNoNXgBTj4JVj8LW9f0ukk2neNP+e3DXUy/rycoiIhVCwQuHFX8ccLHjZ0xi7ZYO/rLilfLkS0REBlTbwWvyXMjUw3MDNx0u2H8imZTp3zZERCpEbQevbD00Hzrofa8x9VkO32+87nuJiFSI2g5eANOOgRcfg60DNwmeMGMSy1Zv4dnV+qNeEZGRpuDVfd/r+T8MuNhxB+4BwG/VdCgiMuIUvCa/Idz3Wn7fgIs1797IgXuNUdOhiEgFUPDK1MGUeYN22oDQdPjw86+wdvOI/l+wiEjNU/ACmHo0vPQ3aF034GInHDiJgsNdT+mPekVERpKCFwz5vtfMyWPYc0y9/qhXRGSEKXhBvO/VMGiXeTPj+Bl78Lu/r+bxf2woU+ZERKSYghdAJgf7zBu00wbA+Ufvx+6NOd553QPcv1R/1isiMhIUvLpNPWpI9732Hd/ET//lSCbv1sCi7zzIfz/yqodBi4jITqbg1W3qMeF9CLWvvcY2cMv7j2DOPrvzkZsf4Ru/X7aTMyciIkkKXt32ngPZxiEFL4CxDVm+d95hnHLwXly5+En+9RdPUNC/zouIlIUeDdwtk4N9Dh+000ZSfTbN1941h4mj6/jWfc/x0sY2/u87DqEuk96JGRUREQWvpKlHwW+vgC1roGnCkD6SShmXvWUGe42t59/veIq/v7SJM17fzCkH78WUcY07OcMiIrVJzYZJU48O78/fv00fMzP++Y2v4f+d/XoasmmuuuMpjv7C3fzTf97Pt+57jhc3tO2EzIqI1C5zr/77NHPnzvUlS5bseEL5TrhqX5j9bjjlS9udzIq1rfzisX/wi7+u4olVGzGDQ/cdx/84aBJvOmAP9pvQhJnteH5FRLaTmT3s7nNHOh/bS8Gr2I1nwMZ/wAcfGJbknl29mdsfXcXtj67i6Zc2ATBlXAMLXrcHbzpgIkfsN4GGnO6RiUh5KXhVgGENXvf+B/z2c/CxpTBq4vCkGa1c18o9f1/N755+mfuXrmVrZ55cJsW8aeOYsdcY9pvYxH4TRzFtQhPjm3KqnYnITlPtwUsdNopNi7/3ev4+OOifhjXpKeMaOefwfTnn8H1p68zz0PJ13PP0au5fuobvLFtOR77Qs+yY+gz7TRzF1PGNTBpbz6TR9ewxpo5JY+rZY3Qde4yuV41NRGqWglexvQ6B3Kjwe69hDl5J9dk0R0+fyNHTQ+0uX3BaXmll2ZotLFu9hWWrN7Ns9RYeWv4KL29qozP/6hpyLp3CCdO7K9DdS+3emGXKuEam7N7IPuPCq3lcA1N2bySTNrryTr7gdBW63wu4w+j6DKPrs4yuz5BNqz+PiFQmBa9i6Szsc8SQf6w8bKtNGfuOb2Lf8U28af++89ydV1o7eXlTGy9vbOeljW28vKmdTW1dmEF342KylXHdlg5WrGvlLytf4fbHVpHfjh9QN2TTjK7PMKYhy7imHPuOa2Tf8Y0xn+F9bEN2+zdaRGQ7KXiVMvUo+M1l0PJw+OeN1MjWQMyMcU05xjXlOGDPbf98V77Aqg1trFjXyguvbKXgTjplZNJGOpUikzLSqRD5Nrd1sbGtk01tXWzcGt/bOlm9qZ17/r6a1Zv6Pohzt8YsY+qzOE6hEAJtwaEQ3+uzKUbVZWjqfuXSNNVlGFWXYXxTjgmj6xjflGP8qDomjArvTbm07veJyIAUvEp57fHwm8vhm8dC43jY90jY9yiYOh/2OGjEg9m2yqRToQlxGH40vaW9ixXrWnl+bSvPr93C8+ta2dqRxwhBNmWQMotFZLR35tnc3sWWji42bO3kH+u30trexaa2Lja1d5VcRzZt1GfS1OfS1GdTYTibpiGbxoye5s6uQqGn+TNfcDJpoy4TPpN8r8umaMplYtBMxyAaxusyKboKBdq7CnTmnY6uAp358MoXPNZsradWm7IwnMuEfDXkQr7qsikasmG8MZuhsS5NUy5DfTY1YCAuFJyOxPq6Ck4h0ZzbnYdMOkU2XmRk0imyaSOTSpGyUO6h/Nnlg35XvkBGzdmCehv2b/0KeO7e8IPl5ffB+ufD9PrdYMphMHF/GD8dJkwP700T+rbbyaDau/Ks29LB2s0drNnc3vO+fmsnbZ152joL8T28tnbmcadPjTETa5ApM/IF7/lce1ff9y0dXbS25/t0iikHM2jKZWjIpanLpOjMF+joCq/2rgJdO+H/MHsuIGKgTadePZwyI52CtBmpGBTTZj0XALlMilw6RS6Toi6TIptJ0dlVYGtnnq0dYV9s7czT1pGnI+/s1phlXGOO3ZuyjGuqY1x8b8ylae/M09ZV6LNP27vydOYd9+7aeqipOyGgb+3Mszle4Gxu72RzWxdb4v7LpVOMrs8wqj7U4EfXZxhVl6Uhl+5pQi8u1eRFQme+QGdX73g23Xvh0X0x0pANFz/5Aj2f6egq9LnQiHu4p+m++0Inm0kl8hVf9RlG12UoOLEMu9jamac1lmV7Z4FcJuSjqS5NQy5DYzZNYy5cGCUvrLrf27sK1GfTvOfwfbfrOKn23oZlDV5mdiLwFSANfNPdryqaXwd8D3gDsBZ4p7svHyzdnRK8im1ogeX3h16ILQ/D2qWQTzSh1Y+FCa+D0XtCug7SuXD/LJ3tHa4bE4Jc44RQo2uK7/W7VV1trlp1dBXYEmuCW9rDSTSbTpFNx5N0PGFnY0B0wsk1vAOxSbQjX+gJqG2dBbZ29AbY1o48rTH9rR1dbInj7Z2FnvTrMuG9+5VNpXqaclMWgnIqBhQH8oVQM+zKF2KtMwy7E0/64eRPzGu+4D2BoDsw5AuOu5N3J1/onZf3UNvLe6jZtHclA2w+jMegkTy5dw9n0saGrZ2s29IRX5280tpR8j5rLp2iLpuiPpsmm7JQW0/FGi29NdvGXIamujSj6rK9QaA+Q30mzdbOPJvaOtnc3hUCXAxyWzu6+tQ8k5eSobacJpe2nv2dzYTabGfB+wSTto48rfGCKZPqPR6y6RDQs+lUTzN79/EBvZ2m2rvybG7Ps7m9k7bOwS+W6uLxEI6pbbu42mtsPX+89Lht+ky3ag9eZWs2NLM0cA1wAtACPGRmt7n7E4nF3gu84u6vNbOzgM8D7yxXHgc0thkOeWd4ARTysGElrFkKa5+BNc/Amr+H8UIn5DugqyO85ztDoMt3lE7bUpCpTwS6usRwLvxpcE8wTATGTF3vtEyc3v3uBejcCl3t0BXfu8fNwudSMbimMr3vqUzITyoDqTRYune4zzrqYr6685MpkV42pAWJWqn1Dlvq1euwVHh5IZwNvJB45ZOFViLN5HsqsZ7u8TQ5S5HLpdi9LgNjckWfS+YzClUDIPGOARmwXHdb3fYcUf0rXqcVbU8VKBScjW2dtHbkqcukYs0z3XPSrxWd+XCxtCkG2XTKepuXc6E5PFkm+VjrbI0tBa0dvRdYydpw8iKoVpXzntdhwFJ3XwZgZjcDpwHJ4HUacHkc/i/g62ZmXoltm6k07D41vKYfP7TPdG6F1rXhj39b14QHX25ZE6bl24uCXcerX10d0L45Md7eGxi7Ol4dIFMZyDSEYJON75n6EAjynVDoCq98Zwy4XSFAFPJhuufDsjII6w26NtDJpPv3DF40HOcNpayT67EUfesXxcsOFCiKgn/P8tZnUv/LFc/rKwXsFl8l00l+LlkGpfLYvZ09FxoDbFZyueJt8gJ9LkKcoR/ffbYz0ThZfGHTvS3xQixrxm6WYrd48dSzn3teiXGMtKUYZSlGGX33cX/7YPSesPDnQ9uGXUw5g9dkYGVivAWY198y7t5lZhuA8cCa4sTM7ALgAoB99tlnZ+R3+GUbQg1ubPPOW4d7CGCWDrWh4UivkC8Kou2J9+4AmgiAhcR48rqjz0k7fmEL+RgwuwjdFWPAtO5aWFFgMCtx8i9x8ug+KXRP63PCKDpxlAok3eM9NZ6ik6fTT5p5hhZQSp2QSq0rceItfhXy9G+A671X7RPoU3aDBdlS8wYKlP3tr1IBoWd7S+zXZDoDrae/bXpVEEyU94D6Ka9XBdTi/ZU47rqP8+KLj+I89AlmyWFKb1fj+EHyvuuq2t6G7n49cD2Ee14jnJ3KYRZqWMOZXjoTA6Ee8SIilaGcDaYvAFMS481xWsllzCwDjCV03BAREelRzuD1EDDdzKaZWQ44C7itaJnbgIVx+O3AXRV5v0tEREZU2ZoN4z2sDwF3ErrKf9vdHzezK4Al7n4b8C3gRjNbCqwjBDgREZE+ynrPy90XA4uLpn02MdwGnFnOPImISPWp3R8JiIhI1VLwEhGRqqPgJSIiVUfBS0REqs4u8a/yZrYaeH47Pz6BEv/gUaNUFn2pPPpSefTaFcpiX3efONKZ2F67RPDaEWa2pJr/WXk4qSz6Unn0pfLopbIYeWo2FBGRqqPgJSIiVUfBK/65rwAqi2Iqj75UHr1UFiOs5u95iYhI9VHNS0REqo6Cl4iIVJ2aDV5mdqKZPW1mS83skpHOT7mZ2bfN7GUz+1ti2jgz+7WZPRPfdx/JPJaLmU0xs7vN7Akze9zMPhKn12p51JvZg2b211gen4vTp5nZn+J35kfx0UY1w8zSZvYXM/tFHK/p8hhpNRm8zCwNXAOcBMwA3mVmM0Y2V2V3A3Bi0bRLgN+6+3Tgt3G8FnQBH3X3GcDhwAfj8VCr5dEOHOvuhwCzgRPN7HDg88CX3f21wCvAe0cwjyPhI8CTifFaL48RVZPBCzgMWOruy9y9A7gZOG2E81RW7v57wjPTkk4DvhuHvwucXtZMjRB3X+Xuf47DmwgnqMnUbnm4u2+Oo9n4cuBY4L/i9JopDwAzawZOAb4Zx40aLo9KUKvBazKwMjHeEqfVuknuvioOvwhMGsnMjAQzmwrMAf5EDZdHbCJ7BHgZ+DXwLLDe3bviIrX2nbkauBgoxPHx1HZ5jLhaDV4yCA+/oaip31GY2SjgJ8D/cveNyXm1Vh7unnf32UAzoaXigBHO0ogxs1OBl9394ZHOi/Qq65OUK8gLwJTEeHOcVuteMrO93H2Vme1FuOquCWaWJQSum9z9p3FyzZZHN3dfb2Z3A0cAu5lZJtY2auk7Mx94q5mdDNQDY4CvULvlURFqteb1EDA99hbKAWcBt41wnirBbcDCOLwQ+O8RzEvZxPsX3wKedPf/SMyq1fKYaGa7xeEG4ATCfcC7gbfHxWqmPNz9UndvdvephHPFXe5+NjVaHpWiZv9hI15FXQ2kgW+7+5UjnKWyMrMfAgsIj3Z4CbgMuBW4BdiH8IiZd7h7caeOXY6ZHQXcCzxG7z2NTxLue9ViecwidEBIEy5wb3H3K8xsP0LnpnHAX4D3uHv7yOW0/MxsAfAxdz9V5TGyajZ4iYhI9arVZkMREaliCl4iIlJ1FLxERKTqKHiJiEjVUfASEZGqo+AlIiJVR8FLRESqzv8HjWlnNjjOGGgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = state['losses']\n",
    "train_losses = losses['train_losses']\n",
    "valid_losses = losses['valid_losses']\n",
    "\n",
    "plt.plot(range(len(train_losses)), train_losses)\n",
    "plt.plot(range(len(train_losses)), valid_losses)\n",
    "plt.title('Figure 2: Train Loss and Evaluting Loss of Seq2Seq + Attention Model')\n",
    "plt.legend(['Train Loss', 'Eval Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3R0EEEhmdd8h"
   },
   "source": [
    "#### With test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDc35QyXOy_z"
   },
   "outputs": [],
   "source": [
    "test_x = map_many_elems(test_inp, src_vocab.stoi, seq_type='input', seq_length=58)\n",
    "test_y = map_many_elems(test_out, tgt_vocab.stoi, seq_type='output', seq_length=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LW482sXOy_1"
   },
   "source": [
    "##### - **Vanilla Seq2Seq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wmv-EY99Oy_1",
    "outputId": "b433c976-4ee2-48f6-e67f-0cd6489376f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, hidden_dim=128, z_type=1)\n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128)\n",
    "\n",
    "state_dict = torch.load(MODEL_PATH_SEQ2SEQ)\n",
    "encoder.load_state_dict(state_dict['encoder'])\n",
    "decoder.load_state_dict(state_dict['decoder'])\n",
    "\n",
    "predictions = predict(encoder, decoder, test_x, tgt_vocab.itos)\n",
    "groundtruth = [''.join(t) for t in test_out]\n",
    "\n",
    "accuracy_score(groundtruth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CBWosHZdGPve",
    "outputId": "3642a623-22fd-41b8-9728-cb90a9d04786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cdfhjmqtuw', 'chmruvw', 'aehiklmrwxyz', 'hmprz', 'dgijmnoy', 'abflrsty', 'bjouy', 'clnqrsxz', 'ehptxz', 'fgv']\n",
      "['cdfhjmqtuw', 'chmruvw', 'aehiklmrwxyz', 'hmprz', 'dgijmnoy', 'abflrsty', 'bjouy', 'clnqrsxz', 'ehptxz', 'fgv']\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0:10])\n",
    "print(groundtruth[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ug5XiZrSOy_3"
   },
   "source": [
    "##### - **Seq2Seq + Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p73nIp22Oy_3",
    "outputId": "3b9ca742-13e9-46f1-c946-a832c5108860"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=64, hidden_dim=128, z_type=0)\n",
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), emb_dim=64, hidden_dim=128, attn_size=64)\n",
    "\n",
    "state_dict = torch.load(MODEL_PATH_SEQ2SEQ_ATTENTION)\n",
    "encoder.load_state_dict(state_dict['encoder'])\n",
    "decoder.load_state_dict(state_dict['decoder'])\n",
    "\n",
    "predictions = predict(encoder, decoder, test_x, tgt_vocab.itos)\n",
    "groundtruth = [''.join(t) for t in test_out]\n",
    "\n",
    "accuracy_score(groundtruth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4HK49Fmstnyy",
    "outputId": "d7012068-c2ee-4a66-d753-15ff1677969c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cdfhjmqtuw', 'chmruvw', 'aehiklmrwxyz', 'hmprz', 'dgijmnoy', 'abflrsty', 'bjouy', 'clnqrsxz', 'ehptxz', 'fgv']\n",
      "['cdfhjmqtuw', 'chmruvw', 'aehiklmrwxyz', 'hmprz', 'dgijmnoy', 'abflrsty', 'bjouy', 'clnqrsxz', 'ehptxz', 'fgv']\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0:10])\n",
    "print(groundtruth[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oak7rij-f4J3"
   },
   "source": [
    "### 6. Analysis & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIxUXX1kQBhr"
   },
   "source": [
    "\n",
    "- With validation data set, the accuracy performance of 2 models are equivalent (**99.8%** vs **99.96%**). *Seq2Seq with Attention* overperforms slightly.\n",
    "- With test data set, the accuracy performance of 2 models are also equivalent (**99.36%** vs **99.92%**). *Seq2Seq with Attention* also performs better slightly.\n",
    "- `Figure 1` and `Figure 2` presents the loss at each epoch for *Vanilla Seq2Seq* model and *Seq2Seq with Attention* model. The loss function of *Seq2Seq with Attention* model converges quicker with less number of epoches than the *Vanilla Seq2Seq* model. That means with less computational step, *Seq2Seq with Attention* model will achieve an equivalent performance with *Vanilla Seq2Seq* model.\n",
    "- Besides, one thing we can notice is the evaluating loss is less than training loss at each epoch step. This is unusual and interesting. However, it could be explained as the input and output has a clear formula of transformation that the output is ordered character-deduplicated string of the input. So that training and validating data set are genenerated without any human/third-party error. In this fashion, because there are less number of samples in validating set (5000/20000) and these samples may be not as difficult/long as in the training set, the model performs better in validating data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0ePdx2UQAiQ"
   },
   "source": [
    "**Thank you for reading and happy weekend!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YuJH1MN2QEDs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW3: Sequence to Sequence Models",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
